// GourmetChat.astro - ã‚°ãƒ«ãƒ¡ã‚µãƒãƒ¼ãƒˆãƒãƒ£ãƒƒãƒˆã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
export interface Props {
  apiBaseUrl?: string;
}

const { apiBaseUrl = '' } = Astro.props;
---

<div class="gourmet-chat-container" data-api-base={apiBaseUrl}>
  <!-- HTMLéƒ¨åˆ†ã¯å¤‰æ›´ãªã— -->
</div>

<style>
  /* CSSéƒ¨åˆ†ã¯å¤‰æ›´ãªã— */
</style>

<script src="https://cdn.socket.io/4.7.2/socket.io.min.js"></script>

<script>
  import { i18n } from '../constants/i18n';
  const LANGUAGE_CODE_MAP = {
    ja: { tts: 'ja-JP', stt: 'ja-JP', voice: 'ja-JP-Chirp3-HD-Leda' },
    en: { tts: 'en-US', stt: 'en-US', voice: 'en-US-Studio-O' },
    zh: { tts: 'cmn-CN', stt: 'cmn-CN', voice: 'cmn-CN-Wavenet-A' },
    ko: { tts: 'ko-KR', stt: 'ko-KR', voice: 'ko-KR-Wavenet-A' }
  };
  
  document.addEventListener('DOMContentLoaded', () => {
    const container = document.querySelector('.gourmet-chat-container') as HTMLElement;
    if (!container) return;

    const apiBase = container.dataset.apiBase || '';

    // ã‚¹ãƒ—ãƒ©ãƒƒã‚·ãƒ¥åˆ¶å¾¡
    const splashOverlay = document.getElementById('splashOverlay') as HTMLDivElement;
    const splashVideo = document.getElementById('splashVideo') as HTMLVideoElement;
    function hideSplash() {
      if (splashVideo) splashVideo.loop = false;
      if (splashOverlay) {
        splashOverlay.classList.add('fade-out');
        setTimeout(() => splashOverlay.classList.add('hidden'), 800);
      }
    }
    setTimeout(() => hideSplash(), 10000);

    // DOMè¦ç´ 
    const chatArea = document.getElementById('chatArea')!;
    const userInput = document.getElementById('userInput') as HTMLInputElement;
    const sendBtn = document.getElementById('sendBtn') as HTMLButtonElement;
    const speakerBtn = document.getElementById('speakerBtn') as HTMLButtonElement;
    const reservationBtn = document.getElementById('reservationBtn') as HTMLButtonElement;
    const voiceStatus = document.getElementById('voiceStatus')!;
    const languageSelect = document.getElementById('languageSelect') as HTMLSelectElement;
    const stopBtn = document.getElementById('stopBtn') as HTMLButtonElement;
    const micBtnFloat = document.getElementById('micBtnFloat') as HTMLButtonElement;
    const waitOverlay = document.getElementById('waitOverlay') as HTMLDivElement;
    const waitVideo = document.getElementById('waitVideo') as HTMLVideoElement;
    let waitOverlayTimer: number | null = null;

    // çŠ¶æ…‹å¤‰æ•°
    let currentLanguage: 'ja' | 'en' | 'zh' | 'ko' = 'ja';
    let sessionId: string | null = null;
    let isProcessing = false;
    let currentStage = 'conversation';
    let isRecording = false;
    let mediaRecorder: MediaRecorder | null = null;
    let audioChunks: Blob[] = [];
    let recordingTimer: number | null = null;
    const MAX_RECORDING_TIME = 55000;
    let isTTSEnabled = true;
    let isUserInteracted = false;
    let currentShops: any[] = [];
    let isFromVoiceInput = false;
    
    // ã‚¨ã‚³ãƒ¼å¯¾ç­–ï¼šChatã‚¢ãƒ—ãƒªç”¨ã«ã‚·ãƒ³ãƒ—ãƒ«åŒ–
    let lastAISpeech: string = '';
    let preGeneratedAcks: Map<string, string> = new Map();
    
    // â˜…â˜…â˜… iPhoneç‰ˆã‹ã‚‰ç§»æ¤ï¼šAudioContextã‚’ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³åŒ– â˜…â˜…â˜…
    let globalAudioContext: AudioContext | null = null;
    let audioWorkletNode: AudioWorkletNode | null = null;
    let mediaStream: MediaStream | null = null;
    let socket: any = null;
    let streamingTranscript = '';
    let isStreamingSTT = false;
    let isSendingAudio = true;
    let isAISpeaking = false;
    let currentAISpeech = "";
    
    // â˜…â˜…â˜… iPhoneç‰ˆã‹ã‚‰ç§»æ¤ï¼šVADé–¢é€£å¤‰æ•° â˜…â˜…â˜…
    let analyser: AnalyserNode | null = null;
    let silenceTimer: number | null = null;
    let vadCheckInterval: number | null = null;
    let hasSpoken = false;
    let recordingStartTime = 0;
    
    const ttsPlayer = new Audio();
    const isIOS = /iPhone|iPad|iPod/i.test(navigator.userAgent);
    const isAndroid = /Android/i.test(navigator.userAgent);
    
    // â˜…â˜…â˜… iPhoneç‰ˆã‹ã‚‰ç§»æ¤ï¼šãƒ‡ãƒã‚¤ã‚¹åˆ¥ã®é–¾å€¤è¨­å®š â˜…â˜…â˜…
    const SILENCE_THRESHOLD = isIOS ? 30 : 35; // iPhone: 30, Android: 35
    const SILENCE_DURATION = isIOS ? 2500 : 2000; // iPhone: 2.5ç§’, Android: 2ç§’
    const MIN_RECORDING_TIME = 3000;

    // --- ä¾¿åˆ©é–¢æ•°ç¾¤ ---
    function t(key: string, ...args: any[]): string {
      // @ts-ignore
      const translation = i18n[currentLanguage][key];
      if (typeof translation === 'function') return translation(...args);
      return translation || key;
    }

    (window as any).gourmetI18n = {
      i18n: i18n,
      getCurrentLanguage: () => currentLanguage,
      t: (key: string, ...args: any[]) => t(key, ...args)
    };
    
    function addMessage(role: string, content: string, summary: string | null = null, isInitial: boolean = false) {
      const messageDiv = document.createElement('div');
      messageDiv.className = `message ${role}`;
      if (isInitial) messageDiv.setAttribute('data-initial', 'true');
      const avatar = document.createElement('div');
      avatar.className = 'message-avatar';
      avatar.innerHTML = role === 'assistant' ? 'ğŸ½' : role === 'user' ? 'ğŸ‘¤' : 'âš ';
      const contentDiv = document.createElement('div');
      contentDiv.className = 'message-content';
      const messageText = document.createElement('span');
      messageText.className = 'message-text';
      messageText.textContent = content;
      contentDiv.appendChild(messageText);
      messageDiv.appendChild(avatar);
      const wrapper = document.createElement('div');
      wrapper.appendChild(contentDiv);
      if (summary) {
        const summaryDiv = document.createElement('div');
        summaryDiv.className = 'summary-box';
        summaryDiv.innerHTML = `<strong>ğŸ“ å†…å®¹ç¢ºèª</strong>${summary}`;
        wrapper.appendChild(summaryDiv);
      }
      messageDiv.appendChild(wrapper);
      chatArea.appendChild(messageDiv);
      chatArea.scrollTop = chatArea.scrollHeight;
    }

    function showError(message: string) {
      const errorDiv = document.createElement('div');
      errorDiv.className = 'error-message';
      errorDiv.textContent = message;
      chatArea.appendChild(errorDiv);
      chatArea.scrollTop = chatArea.scrollHeight;
    }

    function showWaitOverlay() {
      waitOverlay.classList.remove('hidden');
      waitVideo.currentTime = 0;
      waitVideo.play().catch(e => console.log('Video err', e));
    }

    function hideWaitOverlay() {
      if (waitOverlayTimer) { clearTimeout(waitOverlayTimer); waitOverlayTimer = null; }
      waitOverlay.classList.add('hidden');
      setTimeout(() => waitVideo.pause(), 500);
    }

    function unlockAudioParams() {
      ttsPlayer.play().then(() => { ttsPlayer.pause(); ttsPlayer.currentTime = 0; }).catch(e => {});
    }

    function enableAudioPlayback() {
      if (!isUserInteracted) {
        isUserInteracted = true;
        const clickPrompt = container.querySelector('.click-prompt');
        if (clickPrompt) clickPrompt.remove();
        unlockAudioParams();
      }
    }

    function showClickPrompt() {
      const prompt = document.createElement('div');
      prompt.className = 'click-prompt';
      prompt.innerHTML = `<p>ğŸ”Š</p><p>${t('clickPrompt')}</p><p>ğŸ”Š</p>`;
      prompt.addEventListener('click', enableAudioPlayback);
      container.style.position = 'relative';
      container.appendChild(prompt);
    }

    function stopCurrentAudio() {
      ttsPlayer.pause();
      ttsPlayer.currentTime = 0;
    }

    function stripMarkdown(text: string): string {
      return text.replace(/\*\*([^*]+)\*\*/g, '$1').replace(/\*([^*]+)\*/g, '$1').replace(/__([^_]+)__/g, '$1').replace(/_([^_]+)_/g, '$1').replace(/^#+\s*/gm, '').replace(/\[([^\]]+)\]\([^)]+\)/g, '$1').replace(/`([^`]+)`/g, '$1').replace(/^(\d+)\.\s+/gm, '$1ç•ªç›®ã€').replace(/\s+/g, ' ').trim();
    }

    // æ­£è¦åŒ–ï¼ˆã‚¨ã‚³ãƒ¼åˆ¤å®šç”¨ï¼šcore_logic.pyæ–¹å¼ï¼‰
    function normalizeText(text: string): string {
      // è¨˜å·ãƒ»ç©ºç™½ã‚’é™¤å»ã—ã€å°æ–‡å­—åŒ–
      return text.replace(/[!?,.ã€ã€‚\sã€€]/g, '').toLowerCase().trim();
    }

    function removeFillers(text: string): string {
      // @ts-ignore
      const pattern = i18n[currentLanguage].patterns.fillers;
      return text.replace(pattern, '');
    }

    function generateFallbackResponse(text: string): string {
      return t('fallbackResponse', text);
    }

    function selectSmartAcknowledgment(userMessage: string): { text: string, logText: string } {
      const messageLower = userMessage.trim();
      // @ts-ignore
      const p = i18n[currentLanguage].patterns;
      if (p.ackQuestions.test(messageLower)) return { text: t('ackConfirm'), logText: `è³ªå•å½¢å¼` };
      if (p.ackLocation.test(messageLower)) return { text: t('ackSearch'), logText: `å ´æ‰€` };
      if (p.ackSearch.test(messageLower)) return { text: t('ackUnderstood'), logText: `æ¤œç´¢` };
      return { text: t('ackYes'), logText: `ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ` };
    }

    function extractShopsFromResponse(text: string): any[] {
      const shops: any[] = [];
      const pattern = /(\d+)\.\s*\*\*([^*]+)\*\*[ï¼š:]\s*([^\n]+)/g;
      let match;
      while ((match = pattern.exec(text)) !== null) {
        const fullName = match[2].trim();
        const description = match[3].trim();
        let name = fullName;
        const nameMatch = fullName.match(/^([^ï¼ˆ(]+)[ï¼ˆ(]([^ï¼‰)]+)[ï¼‰)]/);
        if (nameMatch) name = nameMatch[1].trim();
        const encodedName = encodeURIComponent(name);
        shops.push({ name: name, description: description, category: 'ã‚¤ã‚¿ãƒªã‚¢ãƒ³', hotpepper_url: `https://www.hotpepper.jp/SA11/srchRS/?keyword=${encodedName}`, maps_url: `https://www.google.com/maps/search/${encodedName}`, tabelog_url: `https://tabelog.com/rstLst/?vs=1&sa=&sk=${encodedName}` });
      }
      return shops;
    }

    function updateUILanguage() {
      voiceStatus.innerHTML = t('voiceStatusStopped');
      userInput.placeholder = t('inputPlaceholder');
      micBtnFloat.title = t('btnVoiceInput');
      speakerBtn.title = isTTSEnabled ? t('btnTTSOn') : t('btnTTSOff');
      sendBtn.textContent = t('btnSend');
      reservationBtn.innerHTML = t('btnReservation');
      const waitText = document.querySelector('.wait-text');
      if (waitText) waitText.textContent = t('waitMessage');

      const pageTitle = document.getElementById('pageTitle');
      if (pageTitle) pageTitle.innerHTML = `<img src="/pwa-152x152.png" alt="Logo" class="app-logo" /> ${t('pageTitle')}`;
      const pageSubtitle = document.getElementById('pageSubtitle');
      if (pageSubtitle) pageSubtitle.textContent = t('pageSubtitle');
      const shopListTitle = document.getElementById('shopListTitle');
      if (shopListTitle) shopListTitle.innerHTML = `ğŸ½ ${t('shopListTitle')}`;
      const shopListEmpty = document.getElementById('shopListEmpty');
      if (shopListEmpty) shopListEmpty.textContent = t('shopListEmpty');
      const pageFooter = document.getElementById('pageFooter');
      if (pageFooter) pageFooter.innerHTML = `${t('footerMessage')} âœ¨`;

      const initialMessage = chatArea.querySelector('.message.assistant[data-initial="true"]');
      if (initialMessage) {
        const messageText = initialMessage.querySelector('.message-text');
        if (messageText) messageText.textContent = t('initialGreeting');
      }

      document.dispatchEvent(new CustomEvent('languageChange', { detail: { language: currentLanguage } }));
    }

    async function speakTextGCP(text: string, stopPrevious: boolean = true, autoRestartMic: boolean = false) {
      if (!isTTSEnabled || !text) return;
      if (stopPrevious) ttsPlayer.pause();
      
      const cleanText = stripMarkdown(text);

      try {
        // â˜… AIéŸ³å£°å†ç”Ÿé–‹å§‹
        isAISpeaking = true;
        // â˜…â˜…â˜… Android: ãƒã‚¤ã‚¯OFF â˜…â˜…â˜…
        if (isAndroid && isRecording) {
          stopStreamingSTT();
        }
        
        voiceStatus.innerHTML = t('voiceStatusSynthesizing');
        voiceStatus.className = 'voice-status speaking';
        const langConfig = LANGUAGE_CODE_MAP[currentLanguage];
        const response = await fetch(`${apiBase}/api/tts/synthesize`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ text: cleanText, language_code: langConfig.tts, voice_name: langConfig.voice })
        });
        const data = await response.json();
        if (data.success && data.audio) {
          ttsPlayer.src = `data:audio/mp3;base64,${data.audio}`;
          const playPromise = new Promise<void>((resolve) => {
            ttsPlayer.onended = async () => {
              voiceStatus.innerHTML = t('voiceStatusStopped');
              voiceStatus.className = 'voice-status stopped';
              // â˜… AIéŸ³å£°å†ç”Ÿçµ‚äº†
              isAISpeaking = false;
              
              // â˜…â˜…â˜… AIéŸ³å£°çµ‚äº†å¾Œï¼šãƒã‚¤ã‚¯è‡ªå‹•ONï¼ˆautoRestartMic=trueã®æ™‚ã®ã¿ï¼‰ â˜…â˜…â˜…
              if (autoRestartMic) {
                if (isAndroid) {
                  try {
                    await startStreamingSTT();
                  } catch (error) {
                    showMicPrompt();
                  }
                } else if (!isRecording) {
                  // iPhone: ãƒã‚¤ã‚¯è‡ªå‹•ONã‚’è©¦ã¿ã‚‹
                  try {
                    await startStreamingSTT();
                  } catch (error) {
                    showMicPrompt();
                  }
                }
              }
              
              resolve();
            };
            ttsPlayer.onerror = () => { 
              // â˜… ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚çµ‚äº†
              isAISpeaking = false;
              resolve(); 
            };
          });
          if (isUserInteracted) {
            lastAISpeech = normalizeText(cleanText);
            await ttsPlayer.play();
            await playPromise;
          } else {
            showClickPrompt();
            voiceStatus.innerHTML = t('voiceStatusStopped');
            voiceStatus.className = 'voice-status stopped';
            // â˜… å†ç”Ÿã—ãªã„å ´åˆã‚‚çµ‚äº†
            isAISpeaking = false;
          }
        } else {
          // â˜… å¤±æ•—æ™‚ã‚‚çµ‚äº†
          isAISpeaking = false;
        }
      } catch (error) {
        voiceStatus.innerHTML = t('voiceStatusStopped');
        voiceStatus.className = 'voice-status stopped';
        // â˜… ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚çµ‚äº†
        isAISpeaking = false;
      }
    }

    // â˜…â˜…â˜… ãƒã‚¤ã‚¯ONä¿ƒé€²ãƒ¢ãƒ¼ãƒ€ãƒ« â˜…â˜…â˜…
    function showMicPrompt() {
      const modal = document.createElement('div');
      modal.id = 'mic-prompt-modal';
      modal.style.cssText = `
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        background: rgba(0, 0, 0, 0.8);
        display: flex;
        align-items: center;
        justify-content: center;
        z-index: 10000;
        animation: fadeIn 0.3s ease;
      `;
      
      modal.innerHTML = `
        <div style="
          background: white;
          border-radius: 16px;
          padding: 24px;
          max-width: 90%;
          width: 350px;
          text-align: center;
          box-shadow: 0 8px 32px rgba(0,0,0,0.3);
        ">
          <div style="font-size: 48px; margin-bottom: 16px;">ğŸ¤</div>
          <div style="font-size: 18px; font-weight: 700; margin-bottom: 8px; color: #333;">
            ãƒã‚¤ã‚¯ã‚’ONã«ã—ã¦ãã ã•ã„
          </div>
          <div style="font-size: 14px; color: #666; margin-bottom: 20px;">
            AIã®å›ç­”ãŒçµ‚ã‚ã‚Šã¾ã—ãŸã€‚<br>ç¶šã‘ã¦è©±ã™ã«ã¯ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’ã‚¿ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚
          </div>
          <button id="mic-prompt-btn" style="
            background: linear-gradient(135deg, #10b981 0%, #059669 100%);
            color: white;
            border: none;
            padding: 14px 32px;
            border-radius: 24px;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            box-shadow: 0 4px 12px rgba(16, 185, 129, 0.3);
          ">
            ğŸ¤ ãƒã‚¤ã‚¯ON
          </button>
        </div>
      `;
      
      const style = document.createElement('style');
      style.textContent = `
        @keyframes fadeIn {
          from { opacity: 0; }
          to { opacity: 1; }
        }
      `;
      document.head.appendChild(style);
      document.body.appendChild(modal);
      
      const btn = document.getElementById('mic-prompt-btn');
      btn?.addEventListener('click', async () => {
        modal.remove();
        await toggleRecording();
      });
      
      // 3ç§’å¾Œã«è‡ªå‹•ã§é–‰ã˜ã‚‹
      setTimeout(() => {
        if (document.getElementById('mic-prompt-modal')) {
          modal.remove();
        }
      }, 3000);
    }

    // --- ãƒ­ã‚¸ãƒƒã‚¯é–¢æ•° ---

    // â˜…â˜…â˜… iPhoneç‰ˆã‹ã‚‰ç§»æ¤ï¼šVADåœæ­¢é–¢æ•° â˜…â˜…â˜…
    function stopVAD() {
      if (vadCheckInterval) { clearInterval(vadCheckInterval); vadCheckInterval = null; }
      if (silenceTimer) { clearTimeout(silenceTimer); silenceTimer = null; }
      if (analyser) { analyser.disconnect(); analyser = null; }
      hasSpoken = false;
    }

    // â˜…â˜…â˜… iPhoneç‰ˆã‹ã‚‰ç§»æ¤ï¼šè‡ªå‹•åœæ­¢é–¢æ•° â˜…â˜…â˜…
    function autoStopRecording() {
      console.log('VAD Stop'); stopVAD();
      if (mediaRecorder && mediaRecorder.state === 'recording') mediaRecorder.stop();
      if (recordingTimer) clearTimeout(recordingTimer);
      isRecording = false; micBtnFloat.classList.remove('recording');
    }

    async function toggleRecording() {
      enableAudioPlayback();
      // â˜…â˜…â˜… AIè©±ä¸­ã§ã‚‚å…¥åŠ›æ¬„ã‚’ã‚¯ãƒªã‚¢ â˜…â˜…â˜…
      userInput.value = '';
      
      stopCurrentAudio();
      if (isRecording) { 
        // â˜…â˜…â˜… ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã§åœæ­¢æ™‚ã‚‚ä¸­æ­¢å‡¦ç†ã‚’å®Ÿè¡Œ â˜…â˜…â˜…
        stopAllActivities();
        return; 
      }
      
      if (isStreamingSTT) { await new Promise(r => setTimeout(r, 100));
      await startStreamingSTT(); }
      else { await startLegacyRecording();
      }
    }

    function toggleTTS() {
      if (!isUserInteracted) { enableAudioPlayback();
      return; }
      enableAudioPlayback();
      isTTSEnabled = !isTTSEnabled;
      speakerBtn.innerHTML = isTTSEnabled ? 'ğŸ”Š' : 'ğŸ”‡';
      speakerBtn.title = isTTSEnabled ? t('btnTTSOn') : t('btnTTSOff');
      speakerBtn.className = isTTSEnabled ? 'btn btn-icon btn-speaker' : 'btn btn-icon btn-speaker disabled';
      if (!isTTSEnabled) stopCurrentAudio();
    }

    let mediaStream: MediaStream | null = null;
    // â˜… ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§ä¿æŒ

    // â˜…â˜…â˜… Androidåˆ¤å®š â˜…â˜…â˜…
    const isAndroid = /Android/i.test(navigator.userAgent);
    console.log('[ãƒ‡ãƒã‚¤ã‚¹åˆ¤å®š] isAndroid:', isAndroid);
    // â˜…â˜…â˜… AIéŸ³å£°å†ç”Ÿä¸­ãƒ•ãƒ©ã‚° â˜…â˜…â˜…
    let isAISpeaking = false;
    // AIéŸ³å£°å†ç”Ÿä¸­ã¯true

    // â˜…â˜…â˜… ã‚¨ã‚³ãƒ¼å¯¾ç­–ï¼šæ±ç”¨ãƒãƒ£ãƒƒãƒˆç”¨ï¼ˆAIç™ºè©±ã‚’è¨˜éŒ²ã—ã¦æ¯”è¼ƒï¼‰ â˜…â˜…â˜…
    let currentAISpeech = "";

    function normalizeText(text: string): string {
      // ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–ï¼ˆç©ºç™½ãƒ»è¨˜å·é™¤å»ï¼‰
      return text.replace(/\s+/g, '').replace(/[ã€ã€‚ï¼ï¼Ÿ,.!?]/g, '').toLowerCase();
    }

    // ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªå‘ã‘ã®ã‚¨ã‚³ãƒ¼åˆ¤å®šï¼ˆç›´å‰ã®AIç™ºè©±ã¨ä¸€è‡´ã™ã‚‹ã‹ï¼‰
    function isSemanticEcho(transcript: string, aiText: string): boolean {
      if (!aiText || !transcript) return false;
      const normTranscript = normalizeText(transcript);
      const normAI = normalizeText(aiText);
      
      // å®Œå…¨ä¸€è‡´ã¾ãŸã¯å¼·ã„åŒ…å«é–¢ä¿‚ã®ã¿ãƒã‚§ãƒƒã‚¯
      if (normAI === normTranscript) return true;
      if (normAI.includes(normTranscript) && normTranscript.length > 5) return true;
      
      return false;
    }

    async function startStreamingSTT() {
      try {
        // â˜…â˜…â˜… è¿½åŠ ï¼šè² è·è»½æ¸›ã®ãŸã‚è£ã§å‹•ã„ã¦ã„ã‚‹å‹•ç”»ã‚’ç¢ºå®Ÿã«åœæ­¢ â˜…â˜…â˜…
        if (splashVideo) { splashVideo.pause(); }
        if (waitVideo) { waitVideo.pause(); }

        // å¤ã„éŒ²éŸ³ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
        if (recordingTimer) {
          clearTimeout(recordingTimer);
          recordingTimer = null;
        }
        
        // â˜…â˜…â˜… AudioWorkletNodeã¨AudioContextã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— â˜…â˜…â˜…
        if (audioWorkletNode) {
          audioWorkletNode.port.onmessage = null;
          audioWorkletNode.disconnect();
          audioWorkletNode = null;
        }
        if (globalAudioContext) {
          if (globalAudioContext.state !== 'closed') {
            await globalAudioContext.close();
          }
          globalAudioContext = null;
        }
        
        // â˜…â˜…â˜… MediaStream: æ¯å›æ–°è¦å–å¾— â˜…â˜…â˜…
        if (mediaStream) {
          mediaStream.getTracks().forEach(track => track.stop());
          mediaStream = null;
        }
        
        try {
          // â˜…â˜…â˜… ä¿®æ­£ï¼šiOSã¯å…¨OFFï¼ˆå‡¦ç†è½ã¡é˜²æ­¢ï¼‰ã€Android/PCã¯å…¨ONï¼ˆéŸ³è³ªç¢ºä¿ï¼‰ â˜…â˜…â˜…
          const audioConstraints: any = { 
            channelCount: 1,
            echoCancellation: isIOS ? false : true,
            noiseSuppression: isIOS ? false : true,
            autoGainControl: isIOS ? false : true
          };
          
          console.log('[Microphone] Constraints:', audioConstraints);

          mediaStream = await navigator.mediaDevices.getUserMedia({ 
            audio: audioConstraints
          });
        } catch (micError: any) {
          // ãƒã‚¤ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ãªå‡¦ç†
          let errorMessage = '';
          if (micError.name === 'NotAllowedError' || micError.name === 'PermissionDeniedError') {
            errorMessage = 'ãƒã‚¤ã‚¯ã®ä½¿ç”¨ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n\nã€è§£æ±ºæ–¹æ³•ã€‘\n1. ãƒ–ãƒ©ã‚¦ã‚¶ã®è¨­å®šã‹ã‚‰ãƒã‚¤ã‚¯ã‚’è¨±å¯\n2. iPhoneã®ã€Œè¨­å®šã€â†’ã€ŒSafariã€â†’ã€Œãƒã‚¤ã‚¯ã€ã‚’ONã«';
          } else if (micError.name === 'NotFoundError') {
            errorMessage = 'ãƒã‚¤ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒã‚¤ã‚¹ã«ãƒã‚¤ã‚¯ãŒæ¥ç¶šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚';
          } else if (micError.name === 'NotReadableError' || micError.name === 'AbortError') {
            errorMessage = 'ãƒã‚¤ã‚¯ãŒä»–ã®ã‚¢ãƒ—ãƒªã§ä½¿ç”¨ä¸­ã§ã™ã€‚\n\nã€è§£æ±ºæ–¹æ³•ã€‘\n1. ä»–ã®ãƒ–ãƒ©ã‚¦ã‚¶ã‚¿ãƒ–ã‚’é–‰ã˜ã‚‹\n2. ä»–ã®ã‚¢ãƒ—ãƒªã‚’çµ‚äº†\n3. iPhoneã‚’å†èµ·å‹•';
          } else {
            errorMessage = `ãƒã‚¤ã‚¯ã‚¨ãƒ©ãƒ¼: ${micError.message}\n\nãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚`;
          }
          
          addMessage('system', errorMessage);
          throw micError;
        }
        
        // â˜…â˜…â˜… AudioContextã‚’ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ¬ãƒ¼ãƒˆã§ä½œæˆ â˜…â˜…â˜…
        try {
          // sampleRateæŒ‡å®šãªã— = ãƒ–ãƒ©ã‚¦ã‚¶ã®ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ¬ãƒ¼ãƒˆï¼ˆ44.1kHzã¾ãŸã¯48kHzï¼‰
          globalAudioContext = new AudioContext();
        } catch (contextError: any) {
          addMessage('system', 'AudioContextä½œæˆã‚¨ãƒ©ãƒ¼ã€‚ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚');
          throw contextError;
        }
        
        // ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—
        const targetSampleRate = 16000;
        const nativeSampleRate = globalAudioContext.sampleRate;
        const downsampleRatio = nativeSampleRate / targetSampleRate;
        console.log(`[Audio] Native: ${nativeSampleRate}Hz, Target: ${targetSampleRate}Hz, Ratio: ${downsampleRatio}`);
        
        // â˜…â˜…â˜… AudioContextã®çŠ¶æ…‹ç¢ºèª â˜…â˜…â˜…
        if (globalAudioContext.state === 'closed') {
          addMessage('system', 'AudioContextãŒã‚¯ãƒ­ãƒ¼ã‚ºã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚');
          throw new Error('AudioContext is closed');
        }
        
        const source = globalAudioContext.createMediaStreamSource(mediaStream);
        // â˜…â˜…â˜… AudioWorkletã‚’ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã§ç™»éŒ²ï¼ˆå‹•çš„ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰ â˜…â˜…â˜…
        const audioProcessorCode = `
class AudioProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    // â˜… ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºï¼š16kHzã§ç´„1ç§’åˆ†ç¢ºä¿ï¼ˆ16000ï¼‰
    this.bufferSize = 16000;
    this.buffer = new Int16Array(this.bufferSize); 
    this.writeIndex = 0;
    
    // â˜… å¤–éƒ¨ã‹ã‚‰æ³¨å…¥ã•ã‚ŒãŸæ¯”ç‡ã‚’ä½¿ç”¨ï¼ˆå‹•çš„ã«æ¯”ç‡ã‚’æ±ºå®šï¼‰
    this.ratio = ${downsampleRatio}; 
    this.inputSampleCount = 0;
    
    // â˜… é€ä¿¡ãƒˆãƒªã‚¬ãƒ¼é–¾å€¤ï¼ˆ0.5ç§’åˆ† = 8000ã‚µãƒ³ãƒ—ãƒ«ï¼‰
    // ã“ã‚Œã«ã‚ˆã‚Šé€šä¿¡å›æ•°ãŒæ¸›ã‚Šã€ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã®è² è·ãŒä¸‹ãŒã‚‹
    this.flushThreshold = 8000;
  }

  process(inputs, outputs, parameters) {
    const input = inputs[0];
    if (!input || input.length === 0) return true;
    
    const channelData = input[0];
    if (!channelData || channelData.length === 0) return true;
    
    // â˜… ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‡¦ç† (Nearest Neighbor for performance)
    for (let i = 0; i < channelData.length; i++) {
      this.inputSampleCount++;
      if (this.inputSampleCount >= this.ratio) {
        this.inputSampleCount -= this.ratio;

        // ãƒãƒƒãƒ•ã‚¡æ›¸ãè¾¼ã¿
        if (this.writeIndex < this.bufferSize) {
          const s = Math.max(-1, Math.min(1, channelData[i]));
          const int16Value = s < 0 ? s * 0x8000 : s * 0x7FFF;
          this.buffer[this.writeIndex++] = int16Value;
        }

        // é–¾å€¤ã‚’è¶…ãˆãŸã‚‰é€ä¿¡
        if (this.writeIndex >= this.flushThreshold) {
          this.flush();
        }
      }
    }
    return true;
  }
  
  flush() {
    if (this.writeIndex === 0) return;

    // ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦é€ä¿¡
    const chunk = this.buffer.slice(0, this.writeIndex);
    this.port.postMessage({ audioChunk: chunk }, [chunk.buffer]);
    
    // ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
    this.writeIndex = 0;
  }
}
registerProcessor('audio-processor', AudioProcessor);
`;
        
        try {
          const blob = new Blob([audioProcessorCode], { type: 'application/javascript' });
          const processorUrl = URL.createObjectURL(blob);
          await globalAudioContext.audioWorklet.addModule(processorUrl);
          URL.revokeObjectURL(processorUrl);
        } catch (workletError: any) {
          addMessage('system', `éŸ³å£°å‡¦ç†ã®åˆæœŸåŒ–ã«å¤±æ•—: ${workletError.message || workletError}`);
          throw workletError;
        }
        
        // â˜…â˜…â˜… AudioWorkletNodeä½œæˆ â˜…â˜…â˜…
        audioWorkletNode = new AudioWorkletNode(globalAudioContext, 'audio-processor');
        audioWorkletNode.port.onmessage = (event) => {
          const { audioChunk } = event.data;
          
          if (!isSendingAudio || !socket || !socket.connected) return;
          
          // â˜…â˜…â˜… ä¿®æ­£ï¼šFileReaderã‚’ä½¿ã£ã¦éåŒæœŸãƒ»çˆ†é€Ÿã§Base64åŒ–ã™ã‚‹ â˜…â˜…â˜…
          // ã“ã‚Œã«ã‚ˆã‚Šãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã®ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼ˆï¼éŸ³é£›ã³ï¼‰ã‚’é˜²ã
          const blob = new Blob([audioChunk], { type: 'application/octet-stream' });
          const reader = new FileReader();
          
          reader.onload = () => {
            // resultã¯ "data:application/octet-stream;base64,AAAA..." å½¢å¼
            const result = reader.result as string;
            const base64 = result.split(',')[1]; // ãƒ˜ãƒƒãƒ€é™¤å»
            
            socket.emit('audio_chunk', { 
              chunk: base64,
              sample_rate: 16000
            });
          };
          
          reader.readAsDataURL(blob);
        };
        
        source.connect(audioWorkletNode);
        audioWorkletNode.connect(globalAudioContext.destination);
        
        // â˜…â˜…â˜… VADï¼ˆç„¡éŸ³æ¤œçŸ¥ï¼‰ã®åˆæœŸåŒ– â˜…â˜…â˜…
        analyser = globalAudioContext.createAnalyser();
        analyser.fftSize = 512;
        source.connect(analyser);
        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        hasSpoken = false; recordingStartTime = Date.now();
        vadCheckInterval = window.setInterval(() => {
          if (!analyser || !isRecording) return;
          if (Date.now() - recordingStartTime < MIN_RECORDING_TIME) return;
          analyser.getByteFrequencyData(dataArray);
          const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;
          
          if (average > SILENCE_THRESHOLD) { hasSpoken = true; if (silenceTimer) clearTimeout(silenceTimer); voiceStatus.innerHTML = t('voiceStatusRecording'); }
          else if (hasSpoken && !silenceTimer) { voiceStatus.innerHTML = t('voiceStatusWaiting'); silenceTimer = window.setTimeout(() => { stopStreamingSTT(); }, SILENCE_DURATION); }
        }, 100);
        // â˜…â˜…â˜… éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ â˜…â˜…â˜…
        
        // â˜…â˜…â˜… WebSocketã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒªã‚»ãƒƒãƒˆ â˜…â˜…â˜…
        if (socket && socket.connected) {
          socket.emit('stop_stream');
          await new Promise(resolve => setTimeout(resolve, 100));
        }
        
        socket.emit('start_stream', { 
          language_code: LANGUAGE_CODE_MAP[currentLanguage].stt,
          sample_rate: 16000  // 16kHzã§é€ä¿¡
        });
        // â˜…â˜…â˜… isSendingAudioã‚’trueã« â˜…â˜…â˜…
        isSendingAudio = true;
        
        isRecording = true; micBtnFloat.classList.add('recording');
        voiceStatus.innerHTML = t('voiceStatusListening');
        
        // éŒ²éŸ³ã‚¿ã‚¤ãƒãƒ¼è¨­å®š
        recordingTimer = window.setTimeout(() => { 
          if (isRecording) { 
            stopStreamingSTT(); 
            addMessage('system', t('recordingTimeLimit')); 
          } 
        }, MAX_RECORDING_TIME);
      } catch (error: any) {
        // ã‚¨ãƒ©ãƒ¼æ™‚ã¯MediaStreamã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆæ¬¡å›ã¯å†å–å¾—ï¼‰
        if (mediaStream) {
          mediaStream.getTracks().forEach(track => track.stop());
          mediaStream = null;
        }
        
        // ã™ã§ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºæ¸ˆã¿ã§ãªã„å ´åˆã®ã¿è¡¨ç¤º
        if (!error.message?.includes('ãƒã‚¤ã‚¯')) {
          addMessage('system', `${t('micAccessError')} ${error.message || 'Unknown error'}`);
        }
      }
    }

    function stopStreamingSTT() {
      stopVAD();
      // éŒ²éŸ³ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
      if (recordingTimer) {
        clearTimeout(recordingTimer);
        recordingTimer = null;
      }
      
      // â˜…â˜…â˜… å…¨ãƒ‡ãƒã‚¤ã‚¹å…±é€š: å®Œå…¨åœæ­¢ â˜…â˜…â˜…
      
      // AudioWorkletNodeåœæ­¢
      if (audioWorkletNode) {
        audioWorkletNode.port.onmessage = null;
        audioWorkletNode.disconnect();
        audioWorkletNode = null;
      }
      
      // AudioContextåœæ­¢
      if (globalAudioContext && globalAudioContext.state !== 'closed') {
        globalAudioContext.close();
        globalAudioContext = null;
      }
      
      // MediaStreamåœæ­¢ï¼ˆæ¯å›ç ´æ£„ï¼‰
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream = null;
      }
      
      // WebSocketåœæ­¢
      if (socket && socket.connected) {
        socket.emit('stop_stream');
      }
      
      // éŸ³å£°é€ä¿¡ãƒ•ãƒ©ã‚°ã‚’åœæ­¢
      isSendingAudio = false;
      isRecording = false; 
      micBtnFloat.classList.remove('recording');
    }

    async function startLegacyRecording() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: { channelCount: 1, sampleRate: 16000, echoCancellation: true, noiseSuppression: true } });
        mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' });
        audioChunks = []; hasSpoken = false; recordingStartTime = Date.now();
        globalAudioContext = new AudioContext();
        const source = globalAudioContext.createMediaStreamSource(stream);
        analyser = globalAudioContext.createAnalyser();
        analyser.fftSize = 512;
        source.connect(analyser);
        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        vadCheckInterval = window.setInterval(() => {
          if (!analyser || !isRecording) return;
          if (Date.now() - recordingStartTime < MIN_RECORDING_TIME) return;
          analyser.getByteFrequencyData(dataArray);
          const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;
          if (average > SILENCE_THRESHOLD) { hasSpoken = true; if (silenceTimer) clearTimeout(silenceTimer); voiceStatus.innerHTML = t('voiceStatusRecording'); }
          else 
          if (hasSpoken && !silenceTimer) { voiceStatus.innerHTML = t('voiceStatusWaiting'); silenceTimer = window.setTimeout(() => { autoStopRecording(); }, SILENCE_DURATION); }
        }, 100);
        mediaRecorder.ondataavailable = (event) => { if (event.data.size > 0) audioChunks.push(event.data); };
        mediaRecorder.onstop = async () => {
          stopVAD(); stream.getTracks().forEach(track => track.stop());
          if (recordingTimer) clearTimeout(recordingTimer);
          if (audioChunks.length > 0) { const audioBlob = new Blob(audioChunks, { type: 'audio/webm' }); await transcribeAudio(audioBlob);
          }
        };
        mediaRecorder.start();
        isRecording = true; micBtnFloat.classList.add('recording'); voiceStatus.innerHTML = t('voiceStatusListening');
        recordingTimer = window.setTimeout(() => { if (isRecording && mediaRecorder && mediaRecorder.state === 'recording') { stopVAD(); mediaRecorder.stop(); isRecording = false; micBtnFloat.classList.remove('recording'); addMessage('system', t('recordingTimeLimit')); } }, MAX_RECORDING_TIME);
      } catch (error) { addMessage('system', `${t('micAccessError')} ${(error as Error).message}`); }
    }

    async function transcribeAudio(audioBlob: Blob) {
      try {
        voiceStatus.innerHTML = t('voiceStatusRecognizing');
        voiceStatus.className = 'voice-status';
        const reader = new FileReader();
        reader.readAsDataURL(audioBlob);
        reader.onloadend = async () => {
          const base64Audio = (reader.result as string).split(',')[1];
          const response = await fetch(`${apiBase}/api/stt/transcribe`, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ audio: base64Audio, language_code: LANGUAGE_CODE_MAP[currentLanguage].stt }) });
          const data = await response.json();

          if (data.success && data.transcript) {
            
            // â˜…â˜…â˜… ã‚¨ã‚³ãƒ¼åˆ¤å®šï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰ â˜…â˜…â˜…
            const normTranscript = normalizeText(data.transcript);
            if (isSemanticEcho(normTranscript, lastAISpeech)) {
                console.log('[ã‚¨ã‚³ãƒ¼æ¤œçŸ¥] ç„¡è¦–:', data.transcript);
                voiceStatus.innerHTML = t('voiceStatusStopped');
                voiceStatus.className = 'voice-status stopped';
                lastAISpeech = '';
                return;
            }

            userInput.value = data.transcript;
            voiceStatus.innerHTML = t('voiceStatusComplete');
            voiceStatus.className = 'voice-status';
            
            addMessage('user', data.transcript);
            // 1. æ—¥æ™‚ãƒã‚§ãƒƒã‚¯
            // @ts-ignore
            if (i18n[currentLanguage].patterns.dateCheck.test(data.transcript)) {
              const msg = t('dateWarningMsg');
              addMessage('assistant', msg);
              
              // â˜…â˜…â˜… AIéŸ³å£°å†ç”Ÿä¸­ã¯éŒ²éŸ³å®Œå…¨åœæ­¢ â˜…â˜…â˜…
              const wasRecording = isRecording;
              if (wasRecording) {
                console.log('[éŒ²éŸ³åˆ¶å¾¡] AIéŸ³å£°å†ç”Ÿã®ãŸã‚ä¸€æ™‚åœæ­¢');
                // LegacyéŒ²éŸ³ã®åœæ­¢
                if (mediaRecorder && mediaRecorder.state === 'recording') {
                  mediaRecorder.stop();
                }
                // WebSocketéŒ²éŸ³ã®åœæ­¢
                stopStreamingSTT();
                isRecording = false;
                micBtnFloat.classList.remove('recording');
              }
              
              if (isTTSEnabled && isUserInteracted) {
                await speakTextGCP(msg, true);
                // éŸ³å£°å†ç”Ÿå®Œäº†ã‚’å¾…ã¤
                await new Promise(r => setTimeout(r, 1000));
                // 1ç§’å¾…æ©Ÿï¼ˆéŸ³å£°ã®ä½™éŸ»ï¼‰
              } else {
                await new Promise(r => setTimeout(r, 2000));
              }
              
              userInput.value = '';
              voiceStatus.innerHTML = t('voiceStatusStopped');
              voiceStatus.className = 'voice-status stopped';
              
              // â˜…â˜…â˜… AIéŸ³å£°å†ç”Ÿå®Œäº†å¾Œã«éŒ²éŸ³å†é–‹ â˜…â˜…â˜…
              if (wasRecording) {
                console.log('[éŒ²éŸ³åˆ¶å¾¡] éŒ²éŸ³å†é–‹');
                setTimeout(() => { toggleRecording().catch(e => {}); }, 500);
              }
              return;
            }

            // 2. æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
            const textLength = data.transcript.trim().replace(/\s+/g, '').length;
            if (textLength < 4) {
                const msg = t('shortMsgWarning');
                addMessage('assistant', msg);
                if (isTTSEnabled && isUserInteracted) {
                  await speakTextGCP(msg, true);
                  await new Promise(r => setTimeout(r, 500));
                } else {
                  await new Promise(r => setTimeout(r, 2000));
                }
                userInput.value = '';
                voiceStatus.innerHTML = t('voiceStatusStopped');
                voiceStatus.className = 'voice-status stopped';
                setTimeout(() => { toggleRecording().catch(e => {}); }, 500);
                return;
            }

            // 3. é€šå¸¸ãƒ•ãƒ­ãƒ¼
            const ack = selectSmartAcknowledgment(data.transcript);
            const preGeneratedAudio = preGeneratedAcks.get(ack.text);
            let firstAckPromise: Promise<void> | null = null;
            if (preGeneratedAudio && isTTSEnabled && isUserInteracted) {
              firstAckPromise = new Promise<void>((resolve) => {
                // â˜…â˜…â˜… preGeneratedAudioå†ç”Ÿæ™‚ã‚‚lastAISpeechã«è¨˜éŒ² â˜…â˜…â˜…
                lastAISpeech = normalizeText(ack.text);
                ttsPlayer.src = `data:audio/mp3;base64,${preGeneratedAudio}`;
  
                ttsPlayer.onended = () => resolve();
                ttsPlayer.play().catch(e => resolve());
              });
            } else if (isTTSEnabled) { firstAckPromise = speakTextGCP(ack.text, false); }
            addMessage('assistant', ack.text);
            (async () => {
                if (firstAckPromise) await firstAckPromise;
                const cleanText = removeFillers(data.transcript);
                const fallbackResponse = generateFallbackResponse(cleanText);
                if (isTTSEnabled && isUserInteracted) await speakTextGCP(fallbackResponse, false);
                addMessage('assistant', fallbackResponse);
 
                setTimeout(async () => {
                  const additionalResponse = t('additionalResponse');
                  if (isTTSEnabled && isUserInteracted) await speakTextGCP(additionalResponse, false);
                  addMessage('assistant', additionalResponse);
                }, 3000);
 
                if (userInput.value.trim()) { isFromVoiceInput = true; sendMessage(); }
            })();
            voiceStatus.innerHTML = t('voiceStatusStopped');
            voiceStatus.className = 'voice-status stopped';
          }
        };
      } catch (error) { console.error('STT Error:', error); }
    }

    function initializeWebSocketSTT() {
      try {
        const wsUrl = apiBase ||
        window.location.origin;
        socket = io(wsUrl);
        socket.on('connect', () => { isStreamingSTT = true; });
        socket.on('disconnect', () => { isStreamingSTT = false; });
        socket.on('transcript', (data: any) => {
          const { text, is_final } = data;
          
          // â˜…â˜…â˜… AIéŸ³å£°å†ç”Ÿä¸­ã¯å…¨ã¦ç„¡è¦– â˜…â˜…â˜…
          if (isAISpeaking) {
            return;
          }
          
          if (is_final) { 
            streamingTranscript = text; 
            handleStreamingSTTComplete(text);
            currentAISpeech = "";
          } else { 
            userInput.value = text;
          }
        });
        socket.on('error', (data: any) => { 
          addMessage('system', `${t('sttError')} ${data.message}`);
          // ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒã‚¤ã‚¯ã‚’åœæ­¢ã—ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒªã‚»ãƒƒãƒˆ
          if (isRecording) {
            stopStreamingSTT();
          }
        });
      } catch (error) { isStreamingSTT = false; }
    }

    async function handleStreamingSTTComplete(transcript: string) {
      // â˜…â˜…â˜… éŸ³å£°èªè­˜å®Œäº†å¾Œã€ãƒã‚¤ã‚¯ã‚’è‡ªå‹•OFF â˜…â˜…â˜…
      stopStreamingSTT();
      voiceStatus.innerHTML = t('voiceStatusComplete');
      voiceStatus.className = 'voice-status';

      // â˜…â˜…â˜… ã‚¨ã‚³ãƒ¼åˆ¤å®šï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰ â˜…â˜…â˜…
      const normTranscript = normalizeText(transcript);
      if (isSemanticEcho(normTranscript, lastAISpeech)) {
          console.log('[ã‚¨ã‚³ãƒ¼æ¤œçŸ¥WS] ç„¡è¦–:', transcript);
          voiceStatus.innerHTML = t('voiceStatusStopped');
          voiceStatus.className = 'voice-status stopped';
          lastAISpeech = '';
          return;
      }

      userInput.value = transcript;
      // â˜…â˜…â˜… æ—¥æ™‚ãƒã‚§ãƒƒã‚¯ â˜…â˜…â˜…
      // @ts-ignore
      if (i18n[currentLanguage].patterns.dateCheck.test(transcript)) {
        const msg = t('dateWarningMsg');
        currentAISpeech = msg;
        addMessage('assistant', msg);
        
        if (isTTSEnabled && isUserInteracted) {
          await speakTextGCP(msg, true, true);
          // â˜… ç¬¬3å¼•æ•° autoRestartMic = true
        } else {
          await new Promise(r => setTimeout(r, 2000));
        }
        
        userInput.value = '';
        voiceStatus.innerHTML = t('voiceStatusStopped');
        voiceStatus.className = 'voice-status stopped';
        return;
      }

      addMessage('user', transcript);
      const textLength = transcript.trim().replace(/\s+/g, '').length;
      if (textLength < 4) {
          const msg = t('shortMsgWarning');
          addMessage('assistant', msg);
          
          // AIéŸ³å£°å†ç”Ÿå‰ã«STTé€ä¿¡åœæ­¢
          isSendingAudio = false;
          if (isTTSEnabled && isUserInteracted) {
            await speakTextGCP(msg, true);
          } else {
            await new Promise(r => setTimeout(r, 2000));
          }
          
          // éŸ³å£°å†ç”Ÿå®Œäº†å¾Œã€å³åº§ã«STTé€ä¿¡å†é–‹
          isSendingAudio = true;
          userInput.value = '';
          voiceStatus.innerHTML = t('voiceStatusStopped');
          voiceStatus.className = 'voice-status stopped';
          return;
      }

      const ack = selectSmartAcknowledgment(transcript);
      const preGeneratedAudio = preGeneratedAcks.get(ack.text);
      let firstAckPromise: Promise<void> |
      null = null;
      if (preGeneratedAudio && isTTSEnabled && isUserInteracted) {
        firstAckPromise = new Promise<void>((resolve) => {
          // â˜…â˜…â˜… preGeneratedAudioå†ç”Ÿæ™‚ã‚‚lastAISpeechã«è¨˜éŒ² â˜…â˜…â˜…
          lastAISpeech = normalizeText(ack.text);
          ttsPlayer.src = `data:audio/mp3;base64,${preGeneratedAudio}`;
          ttsPlayer.onended = () => resolve();
          ttsPlayer.play().catch(e => resolve());
   
        });
      } else if (isTTSEnabled) { firstAckPromise = speakTextGCP(ack.text, false);
      }
      addMessage('assistant', ack.text);

      (async () => {
        try {
          if (firstAckPromise) await firstAckPromise;
          const cleanText = removeFillers(transcript);
          const fallbackResponse = generateFallbackResponse(cleanText);
          if (isTTSEnabled && isUserInteracted) await speakTextGCP(fallbackResponse, false);
          addMessage('assistant', fallbackResponse);
          setTimeout(async () => {
 
            const additionalResponse = t('additionalResponse');
            if (isTTSEnabled && isUserInteracted) await speakTextGCP(additionalResponse, false);
            addMessage('assistant', additionalResponse);
          }, 3000);
          
          if (userInput.value.trim()) {
            isFromVoiceInput = true;
            sendMessage();
          }
        } catch (error) { 
          if (userInput.value.trim()) {
            isFromVoiceInput = true;
            sendMessage();
          }
        }
      })();
      voiceStatus.innerHTML = t('voiceStatusStopped');
      voiceStatus.className = 'voice-status stopped';
    }

    async function sendMessage() {
      let firstAckPromise: Promise<void> |
      null = null; 
      unlockAudioParams();
      const message = userInput.value.trim();
      if (!message || isProcessing) return;
      isProcessing = true; sendBtn.disabled = true;
      micBtnFloat.disabled = true; userInput.disabled = true;

      if (!isFromVoiceInput) {
        addMessage('user', message);
        // @ts-ignore
        if (i18n[currentLanguage].patterns.dateCheck.test(message)) {
             const msg = t('dateWarningMsg');
             if (isTTSEnabled && isUserInteracted) await speakTextGCP(msg, true);
             addMessage('assistant', msg);
             userInput.value = ''; isProcessing = false; sendBtn.disabled = false;
             micBtnFloat.disabled = false; userInput.disabled = false; userInput.focus();
             return;
        }
        const textLength = message.trim().replace(/\s+/g, '').length;
        if (textLength < 4) {
             const msg = t('shortMsgWarning');
             addMessage('assistant', msg);
             if (isTTSEnabled && isUserInteracted) await speakTextGCP(msg, true);
             userInput.value = ''; isProcessing = false; sendBtn.disabled = false;
             micBtnFloat.disabled = false; userInput.disabled = false; userInput.focus();
             return;
        }
        userInput.value = '';
        const ack = selectSmartAcknowledgment(message);
        
        // â˜… ç›¸æ§Œã‚’è¨˜éŒ²ï¼ˆã‚¨ã‚³ãƒ¼åˆ¤å®šç”¨ï¼‰
        currentAISpeech = ack.text;
        
        addMessage('assistant', ack.text);
        if (isTTSEnabled) {
          try {
            const preGeneratedAudio = preGeneratedAcks.get(ack.text);
            if (preGeneratedAudio && isUserInteracted) {
              firstAckPromise = new Promise<void>((resolve) => {
                // â˜…â˜…â˜… preGeneratedAudioå†ç”Ÿæ™‚ã‚‚lastAISpeechã«è¨˜éŒ² â˜…â˜…â˜…
                lastAISpeech = normalizeText(ack.text);
                ttsPlayer.src = `data:audio/mp3;base64,${preGeneratedAudio}`;
    
                ttsPlayer.onended = () => resolve();
                ttsPlayer.play().catch(e => resolve());
              });
            } else { firstAckPromise = speakTextGCP(ack.text, false); }
          } catch (e) {}
        }
        if (firstAckPromise) await firstAckPromise;
        const cleanText = removeFillers(message);
        const fallbackResponse = generateFallbackResponse(cleanText);
        if (isTTSEnabled && isUserInteracted) await speakTextGCP(fallbackResponse, false);
        addMessage('assistant', fallbackResponse);
        setTimeout(async () => {
          const additionalResponse = t('additionalResponse');
          if (isTTSEnabled && isUserInteracted) await speakTextGCP(additionalResponse, false);
          addMessage('assistant', additionalResponse);
        }, 3000);
      }

      const wasVoiceInput = isFromVoiceInput;
      isFromVoiceInput = false;
      if (waitOverlayTimer) clearTimeout(waitOverlayTimer);
      waitOverlayTimer = window.setTimeout(() => { showWaitOverlay(); }, 4000);

      try {
        const response = await fetch(`${apiBase}/api/chat`, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ session_id: sessionId, message: message, stage: currentStage, language: currentLanguage }) });
        const data = await response.json();
        hideWaitOverlay();
        
        // â˜… AIå¿œç­”ã‚’è¨˜éŒ²ï¼ˆã‚¨ã‚³ãƒ¼åˆ¤å®šç”¨ï¼‰
        currentAISpeech = data.response;
        addMessage('assistant', data.response, data.summary);
        stopCurrentAudio();

        if (data.shops && data.shops.length > 0) {
          currentShops = data.shops;
          reservationBtn.disabled = false;
          
          // â˜…â˜…â˜… å…¥åŠ›æ¬„ã‚’ã‚¯ãƒªã‚¢ â˜…â˜…â˜…
          userInput.value = '';
          document.dispatchEvent(new CustomEvent('displayShops', { detail: { shops: data.shops, language: currentLanguage } }));
          const section = document.getElementById('shopListSection');
          if (section) section.classList.add('has-shops');
          // â˜…â˜…â˜… ã‚¹ãƒãƒ›ã®å ´åˆï¼šã‚·ãƒ§ãƒƒãƒ—ã‚«ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« â˜…â˜…â˜…
          if (window.innerWidth < 1024) {
            setTimeout(() => {
              const shopSection = document.getElementById('shopListSection');
              if (shopSection) {
                shopSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
              }
 
            }, 300);
            // DOMãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾…ã¡
          }
          
          (async () => {
            try {
              // â˜… ã‚·ãƒ§ãƒƒãƒ—éŸ³å£°å†ç”Ÿé–‹å§‹
              isAISpeaking = true;
              if (isAndroid && isRecording) {
   
                stopStreamingSTT();
              }
              
              await speakTextGCP(t('ttsIntro'));
              const lines = data.response.split('\n\n');
              let introText = ""; let shopLines = lines;
      
              if (lines[0].includes('ã”å¸Œæœ›ã«åˆã†ãŠåº—') && lines[0].includes('ã”ç´¹ä»‹ã—ã¾ã™')) { introText = lines[0]; shopLines = lines.slice(1); }
              
              let introPart2Promise: Promise<void> | null = null;
              if (introText && isTTSEnabled && isUserInteracted) {
                const preGeneratedIntro = preGeneratedAcks.get(introText);
        
                if (preGeneratedIntro) {
                  introPart2Promise = new Promise<void>((resolve) => {
                    lastAISpeech = normalizeText(introText);
                    ttsPlayer.src = `data:audio/mp3;base64,${preGeneratedIntro}`;
                    ttsPlayer.onended = () => resolve();
                    ttsPlayer.play();
                  });
                } else { introPart2Promise = speakTextGCP(introText, false);
                }
              }

              let firstShopAudioPromise: Promise<string |
              null> | null = null;
              let remainingAudioPromise: Promise<string | null> | null = null;
              const shopLangConfig = LANGUAGE_CODE_MAP[currentLanguage];
              if (shopLines.length > 0 && isTTSEnabled && isUserInteracted) {
                const firstShop = shopLines[0];
                const restShops = shopLines.slice(1).join('\n\n');
                firstShopAudioPromise = (async () => {
                  const cleanText = stripMarkdown(firstShop);
                  const response = await fetch(`${apiBase}/api/tts/synthesize`, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ text: cleanText, language_code: shopLangConfig.tts, voice_name: shopLangConfig.voice }) });
                  const result = await response.json();
        
                  return result.success ? `data:audio/mp3;base64,${result.audio}` : null;
                })();
                if (restShops) {
                  remainingAudioPromise = (async () => {
                    const cleanText = stripMarkdown(restShops);
                    const response = await fetch(`${apiBase}/api/tts/synthesize`, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ text: cleanText, language_code: shopLangConfig.tts, voice_name: shopLangConfig.voice }) });
          
                    const result = await response.json();
                    return result.success ? `data:audio/mp3;base64,${result.audio}` : null;
                  })();
                }
              }

              if (introPart2Promise) await introPart2Promise;
              if (firstShopAudioPromise) {
                const firstShopAudio = await firstShopAudioPromise;
                if (firstShopAudio) {
                  const firstShopText = stripMarkdown(shopLines[0]);
                  lastAISpeech = normalizeText(firstShopText);
                  
                  stopCurrentAudio(); ttsPlayer.src = firstShopAudio;
                  await new Promise<void>((resolve) => { ttsPlayer.onended = () => { voiceStatus.innerHTML = t('voiceStatusStopped'); voiceStatus.className = 'voice-status stopped'; resolve(); }; voiceStatus.innerHTML = t('voiceStatusSpeaking'); voiceStatus.className = 'voice-status speaking'; ttsPlayer.play(); });
                  if (remainingAudioPromise) {
                    const remainingAudio = await remainingAudioPromise;
                    if (remainingAudio) {
                      const restShopsText = stripMarkdown(shopLines.slice(1).join('\n\n'));
                      lastAISpeech = normalizeText(restShopsText);
                      
                      await new Promise(r => setTimeout(r, 500));
                      stopCurrentAudio(); ttsPlayer.src = remainingAudio;
                      await new Promise<void>((resolve) => { ttsPlayer.onended = () => { voiceStatus.innerHTML = 'ğŸ¤ éŸ³å£°èªè­˜: åœæ­¢ä¸­'; voiceStatus.className = 'voice-status stopped'; resolve(); }; voiceStatus.innerHTML = 'ğŸ”Š éŸ³å£°å†ç”Ÿä¸­...'; voiceStatus.className = 'voice-status speaking'; ttsPlayer.play(); });
                    }
                  }
                }
              }
              
              // â˜… ã‚·ãƒ§ãƒƒãƒ—éŸ³å£°å†ç”Ÿçµ‚äº†
              isAISpeaking = false;
            } catch (e) { 
              isAISpeaking = false;
            }
          })();
        } else {
          if (data.response) {
            const extractedShops = extractShopsFromResponse(data.response);
            if (extractedShops.length > 0) {
              currentShops = extractedShops;
              reservationBtn.disabled = false;
              document.dispatchEvent(new CustomEvent('displayShops', { detail: { shops: extractedShops, language: currentLanguage } }));
              const section = document.getElementById('shopListSection');
              if (section) section.classList.add('has-shops');
              speakTextGCP(data.response);
            } else { speakTextGCP(data.response); }
          }
        }
      } catch (error) { console.error('é€ä¿¡ã‚¨ãƒ©ãƒ¼:', error);
      hideWaitOverlay(); showError('ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚'); }
      finally { isProcessing = false; sendBtn.disabled = false; micBtnFloat.disabled = false;
      userInput.disabled = false; if (currentShops.length === 0) userInput.focus(); else userInput.blur();
      }
    }

    // ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼
    function openReservationModal() {
      if (currentShops.length === 0) { showError(t('searchError'));
      return; }
      document.dispatchEvent(new CustomEvent('openReservationModal', { detail: { shops: currentShops } }));
    }

    // åˆæœŸåŒ–é–¢æ•°
    async function initialize() {
        try {
            const response = await fetch(`${apiBase}/api/session/start`, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ user_info: {}, language: currentLanguage }) });
            const data = await response.json(); sessionId = data.session_id;
            addMessage('assistant', t('initialGreeting'), null, true);
            const ackTexts = [t('ackConfirm'), t('ackSearch'), t('ackUnderstood'), t('ackYes'), t('ttsIntro')];
            const langConfig = LANGUAGE_CODE_MAP[currentLanguage];
            const ackPromises = ackTexts.map(async (text) => {
              try {
                const ackResponse = await fetch(`${apiBase}/api/tts/synthesize`, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ text: text, language_code: langConfig.tts, voice_name: langConfig.voice }) });
                const ackData = await ackResponse.json();
                if 
                (ackData.success && ackData.audio) preGeneratedAcks.set(text, ackData.audio);
              } catch (e) {}
            });
            await Promise.all([speakTextGCP(t('initialGreeting')), ...ackPromises]);
            userInput.disabled = false; sendBtn.disabled = false; micBtnFloat.disabled = false; speakerBtn.disabled = false; userInput.focus();
            if (splashOverlay) hideSplash();
            initializeWebSocketSTT();
            updateUILanguage();
        } catch(e) { console.error(e); }
    }

    // ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼ç™»éŒ²
    languageSelect.addEventListener('change', () => { currentLanguage = languageSelect.value as any; updateUILanguage(); });
    sendBtn.addEventListener('click', sendMessage);
    micBtnFloat.addEventListener('click', toggleRecording);
    speakerBtn.addEventListener('click', toggleTTS);
    userInput.addEventListener('keypress', (e) => { if (e.key === 'Enter') sendMessage(); });
    reservationBtn.addEventListener('click', openReservationModal);
    // â˜…â˜…â˜… ã‚½ãƒ•ãƒˆã‚­ãƒ¼ãƒœãƒ¼ãƒ‰è¡¨ç¤ºæ¤œçŸ¥ â˜…â˜…â˜…
    const floatingButtons = document.querySelector('.floating-buttons') as HTMLElement;
    userInput.addEventListener('focus', () => {
      // ãƒ•ã‚©ãƒ¼ã‚«ã‚¹æ™‚ã€å°‘ã—é…å»¶ã—ã¦ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰è¡¨ç¤ºã‚’æ¤œçŸ¥
      setTimeout(() => {
        if (floatingButtons) {
          floatingButtons.classList.add('keyboard-active');
        }
      }, 300);
    });
    userInput.addEventListener('blur', () => {
      // ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ãŒå¤–ã‚ŒãŸã‚‰ãƒœã‚¿ãƒ³ã‚’å…ƒã®ä½ç½®ã«
      if (floatingButtons) {
        floatingButtons.classList.remove('keyboard-active');
      }
    });
    // â˜…â˜…â˜… ä¸­æ­¢å‡¦ç†ï¼ˆå…±é€šé–¢æ•°ï¼‰ â˜…â˜…â˜…
    function stopAllActivities() {
      // â˜…â˜…â˜… ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®LLMå‡¦ç†ã‚’ä¸­æ­¢ â˜…â˜…â˜…
      if (isProcessing) {
        fetch(`${apiBase}/api/cancel`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ session_id: sessionId })
        }).catch(err => console.error('ä¸­æ­¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—:', err));
      }
      
      // éŒ²éŸ³ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
      if (recordingTimer) {
        clearTimeout(recordingTimer);
        recordingTimer = null;
      }
      
      // éŸ³å£°èªè­˜ã‚’åœæ­¢
      if (isRecording) {
        stopStreamingSTT();
      }
      
      // TTSå†ç”Ÿã‚’åœæ­¢
      stopCurrentAudio();
      // å¾…æ©Ÿã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã‚’éè¡¨ç¤º
      waitOverlay.classList.add('hidden');
      if (waitOverlayTimer) {
        clearTimeout(waitOverlayTimer);
        waitOverlayTimer = null;
      }
      
      // ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
      isProcessing = false;
      isAISpeaking = false;
      
      // UIçŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
      voiceStatus.innerHTML = t('voiceStatusStopped');
      voiceStatus.className = 'voice-status stopped';
      // å…¥åŠ›æ¬„ã‚’ã‚¯ãƒªã‚¢ã—ã¦ãƒ•ã‚©ãƒ¼ã‚«ã‚¹
      userInput.value = '';
      userInput.focus();
      // ãƒãƒ£ãƒƒãƒˆç”»é¢ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆã‚¹ãƒãƒ›å¯¾å¿œï¼‰
      if (window.innerWidth < 1024) {
        setTimeout(() => {
          chatArea.scrollIntoView({ behavior: 'smooth', block: 'start' });
        }, 100);
      }
    }
    
    // â˜…â˜…â˜… ä¸­æ­¢ãƒœã‚¿ãƒ³ â˜…â˜…â˜…
    stopBtn.addEventListener('click', () => {
      stopAllActivities();
    });
    // æœ€å¾Œã«åˆæœŸåŒ–ã‚’å®Ÿè¡Œ
    initialize();
  });
</script>
