---
// GourmetChat.astro - グルメサポートチャットコンポーネント
export interface Props {
  apiBaseUrl?: string;
}

const { apiBaseUrl = '' } = Astro.props;
---

<div class="gourmet-chat-container" data-api-base={apiBaseUrl}>
  <div class="chat-header">
    <h2>&#127869; グルメサポートAI</h2>
    <p>お店探しをお手伝いします</p>
  </div>

  <div class="voice-status stopped" id="voiceStatus">
    &#127908; 音声認識: 停止中
  </div>

  <div class="chat-area" id="chatArea">
    <!-- メッセージはJSで動的に追加 -->
  </div>

  <div class="input-area">
    <div class="input-group">
      <input 
        type="text" 
        id="userInput" 
        placeholder="メッセージを入力..."
        disabled
      />
      <button class="btn btn-icon btn-mic" id="micBtn" title="音声入力" disabled>
        &#127908;
      </button>
      <button class="btn btn-icon btn-speaker" id="speakerBtn" title="音声読み上げON" disabled>
        &#128266;
      </button>
      <button class="btn" id="sendBtn" disabled>
        送信
      </button>
    </div>
    <div class="input-actions">
      <button class="btn btn-reservation" id="reservationBtn" disabled>
        &#128222; 予約依頼する
      </button>
    </div>
  </div>
</div>

<style>
  .gourmet-chat-container {
    background: white;
    border-radius: 16px;
    box-shadow: 0 20px 60px rgba(0, 0, 0, 0.15);
    width: 100%;
    max-width: 800px;
    max-height: 600px;
    display: flex;
    flex-direction: column;
    overflow: hidden;
    margin: 0 auto;
  }

  .chat-header {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 20px;
    text-align: center;
  }

  .chat-header h2 {
    font-size: 20px;
    font-weight: 600;
    margin: 0 0 4px 0;
  }

  .chat-header p {
    font-size: 13px;
    opacity: 0.9;
    margin: 0;
  }

  .voice-status {
    padding: 10px 15px;
    text-align: center;
    font-size: 12px;
    border-bottom: 1px solid #e0e0e0;
    font-weight: 500;
  }

  .voice-status.listening {
    background: #e8f5e9;
    color: #2e7d32;
    animation: pulse 2s infinite;
  }

  .voice-status.stopped {
    background: #ffebee;
    color: #c62828;
  }

  .voice-status.speaking {
    background: #e1f5fe;
    color: #0277bd;
    animation: pulse 2s infinite;
  }

  @keyframes pulse {
    0%, 100% { opacity: 1; }
    50% { opacity: 0.8; }
  }

  .chat-area {
    flex: 1;
    overflow-y: auto;
    padding: 20px;
    background: #f7f9fc;
    min-height: 300px;
  }

  .message {
    margin-bottom: 16px;
    display: flex;
    gap: 10px;
  }

  .message.assistant {
    flex-direction: row;
  }

  .message.user {
    flex-direction: row-reverse;
  }

  .message.system {
    justify-content: center;
  }

  .message-avatar {
    width: 36px;
    height: 36px;
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 16px;
    flex-shrink: 0;
  }

  .message.assistant .message-avatar {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
  }

  .message.user .message-avatar {
    background: #e0e7ff;
    color: #667eea;
  }

  .message.system .message-avatar {
    background: #fff3e0;
    color: #f57c00;
  }

  .message-content {
    max-width: 70%;
    padding: 10px 14px;
    border-radius: 12px;
    line-height: 1.5;
    font-size: 14px;
    white-space: pre-wrap;
  }

  .message.assistant .message-content {
    background: white;
    border: 1px solid #e5e7eb;
    color: #1f2937;
  }

  .message.user .message-content {
    background: #667eea;
    color: white;
  }

  .message.system .message-content {
    background: #fff3e0;
    color: #e65100;
    font-size: 12px;
  }

  .summary-box {
    margin-top: 10px;
    padding: 10px;
    background: #fef3c7;
    border-left: 4px solid #f59e0b;
    border-radius: 8px;
    font-size: 13px;
    color: #92400e;
  }

  .summary-box strong {
    display: block;
    margin-bottom: 6px;
    color: #78350f;
  }

  .final-summary {
    margin: 16px 0;
    padding: 16px;
    background: white;
    border: 2px solid #10b981;
    border-radius: 12px;
  }

  .final-summary h3 {
    color: #065f46;
    margin: 0 0 10px 0;
    font-size: 16px;
  }

  .final-summary-content {
    white-space: pre-wrap;
    line-height: 1.7;
    color: #1f2937;
    font-size: 13px;
  }

  .input-area {
    padding: 16px;
    background: white;
    border-top: 1px solid #e5e7eb;
  }

  .input-group {
    display: flex;
    gap: 10px;
    align-items: center;
    margin-bottom: 10px;
  }

  #userInput {
    flex: 1;
    padding: 10px 14px;
    border: 2px solid #e5e7eb;
    border-radius: 20px;
    font-size: 14px;
    outline: none;
    transition: border-color 0.2s;
  }

  #userInput:focus {
    border-color: #667eea;
  }

  .btn {
    padding: 10px 16px;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    border: none;
    border-radius: 20px;
    font-size: 14px;
    font-weight: 600;
    cursor: pointer;
    transition: transform 0.2s, box-shadow 0.2s;
  }

  .btn:hover:not(:disabled) {
    transform: translateY(-2px);
    box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
  }

  .btn:disabled {
    opacity: 0.5;
    cursor: not-allowed;
  }

  .btn-secondary {
    background: #f3f4f6;
    color: #374151;
    font-size: 12px;
    padding: 8px 12px;
  }

  .btn-secondary:hover:not(:disabled) {
    background: #e5e7eb;
    box-shadow: none;
    transform: none;
  }

  .btn-reservation {
    background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%);
    color: white;
    font-size: 13px;
    padding: 10px 16px;
    font-weight: 600;
    box-shadow: 0 4px 12px rgba(245, 158, 11, 0.3);
  }

  .btn-reservation:hover:not(:disabled) {
    box-shadow: 0 6px 16px rgba(245, 158, 11, 0.4);
    transform: translateY(-1px);
  }

  .btn-reservation:disabled {
    opacity: 0.5;
    cursor: not-allowed;
  }

  .btn-icon {
    padding: 10px;
    width: 40px;
    height: 40px;
    display: flex;
    align-items: center;
    justify-content: center;
  }

  .btn-mic {
    background: #10b981;
  }

  .btn-mic:hover:not(:disabled) {
    box-shadow: 0 4px 12px rgba(16, 185, 129, 0.4);
  }

  .btn-mic.recording {
    background: #ef4444;
    animation: pulse-btn 1.5s infinite;
  }

  @keyframes pulse-btn {
    0%, 100% { opacity: 1; }
    50% { opacity: 0.7; }
  }

  .btn-speaker {
    background: #f59e0b;
  }

  .btn-speaker:hover:not(:disabled) {
    box-shadow: 0 4px 12px rgba(245, 158, 11, 0.4);
  }

  .btn-speaker.disabled {
    background: #9ca3af;
  }

  .input-actions {
    display: flex;
    justify-content: flex-end;
  }

  .loading {
    display: inline-block;
    width: 20px;
    height: 20px;
    border: 3px solid #e5e7eb;
    border-radius: 50%;
    border-top-color: #667eea;
    animation: spin 1s ease-in-out infinite;
  }

  @keyframes spin {
    to { transform: rotate(360deg); }
  }

  .error-message {
    background: #fee2e2;
    color: #b91c1c;
    padding: 10px;
    border-radius: 8px;
    margin: 10px 0;
    font-size: 13px;
    text-align: center;
  }

  /* クリックプロンプト */
  .click-prompt {
    position: absolute;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    background: rgba(0, 0, 0, 0.8);
    color: white;
    padding: 20px;
    border-radius: 12px;
    text-align: center;
    z-index: 100;
    cursor: pointer;
  }

  .click-prompt p {
    margin: 5px 0;
  }
</style>

<!-- Socket.IO クライアントライブラリ（WebSocket Streaming STT用） -->
<script src="https://cdn.socket.io/4.7.2/socket.io.min.js"></script>

<script>
  // グルメチャット機能
  document.addEventListener('DOMContentLoaded', () => {
    const container = document.querySelector('.gourmet-chat-container') as HTMLElement;
    if (!container) return;

    const apiBase = container.dataset.apiBase || '';
    
    // DOM要素
    const chatArea = document.getElementById('chatArea')!;
    const userInput = document.getElementById('userInput') as HTMLInputElement;
    const sendBtn = document.getElementById('sendBtn') as HTMLButtonElement;
    const micBtn = document.getElementById('micBtn') as HTMLButtonElement;
    const speakerBtn = document.getElementById('speakerBtn') as HTMLButtonElement;
    const reservationBtn = document.getElementById('reservationBtn') as HTMLButtonElement;
    const voiceStatus = document.getElementById('voiceStatus')!;

    // 状態
    let sessionId: string | null = null;
    let isProcessing = false;
    let currentStage = 'conversation';
    let isRecording = false;
    let mediaRecorder: MediaRecorder | null = null;
    let audioChunks: Blob[] = [];
    let recordingTimer: number | null = null;
    const MAX_RECORDING_TIME = 55000; // 55秒（APIの60秒制限より短く設定）
    let isTTSEnabled = true;
    let currentAudio: HTMLAudioElement | null = null;
    let isUserInteracted = false;
    let currentShops: any[] = []; // 現在表示中のショップリスト
    let isFromVoiceInput = false; // 音声入力からの送信かどうかを追跡

    // 事前生成音声（案1: 高速応答用）
    let preGeneratedAcks: Map<string, string> = new Map(); // key: テキスト, value: base64 MP3

    // 無音検出用
    let audioContext: AudioContext | null = null;
    let analyser: AnalyserNode | null = null;
    let silenceTimer: number | null = null;
    let vadCheckInterval: number | null = null;
    let hasSpoken = false; // 音声が検出されたかどうか
    let recordingStartTime = 0; // 録音開始時刻
    const SILENCE_THRESHOLD = 35; // 無音判定の閾値（0-255）※環境ノイズを無視
    const SILENCE_DURATION = 2000; // 無音が続いたら停止するミリ秒（2秒）
    const MIN_RECORDING_TIME = 3000; // 最小録音時間（3秒）- マイク初期化音を無視

    // WebSocket Streaming STT用（案2: 完全実装）
    let socket: any = null;
    let audioWorkletNode: AudioWorkletNode | null = null;
    let streamingTranscript = ''; // リアルタイム認識結果
    let isStreamingSTT = false; // Streaming STT使用フラグ

    // ユーザーインタラクション検出
    function enableAudioPlayback() {
      if (!isUserInteracted) {
        isUserInteracted = true;
        const clickPrompt = container.querySelector('.click-prompt');
        if (clickPrompt) clickPrompt.remove();
      }
    }

    // クリックプロンプト表示
    function showClickPrompt() {
      const prompt = document.createElement('div');
      prompt.className = 'click-prompt';
      prompt.innerHTML = `
        <p>&#128266;</p>
        <p>音声を再生するには、画面をクリックしてください</p>
        <p>&#128266;</p>
      `;
      prompt.addEventListener('click', enableAudioPlayback);
      container.style.position = 'relative';
      container.appendChild(prompt);
    }

    // 音声停止
    function stopCurrentAudio() {
      if (currentAudio) {
        currentAudio.pause();
        currentAudio.src = '';
        currentAudio = null;
      }
    }

    // マークダウン記号を除去（TTS用）
    function stripMarkdown(text: string): string {
      return text
        // **太字** や *斜体* を除去
        .replace(/\*\*([^*]+)\*\*/g, '$1')
        .replace(/\*([^*]+)\*/g, '$1')
        // __太字__ や _斜体_ を除去
        .replace(/__([^_]+)__/g, '$1')
        .replace(/_([^_]+)_/g, '$1')
        // # 見出し を除去
        .replace(/^#+\s*/gm, '')
        // [リンクテキスト](URL) を リンクテキスト に
        .replace(/\[([^\]]+)\]\([^)]+\)/g, '$1')
        // `コード` を除去
        .replace(/`([^`]+)`/g, '$1')
        // 番号リストの番号を読みやすく
        .replace(/^(\d+)\.\s+/gm, '$1番目、')
        // 余分な空白を整理
        .replace(/\s+/g, ' ')
        .trim();
    }

    // GCP TTS音声合成
    async function speakTextGCP(text: string, stopPrevious: boolean = true) {
      console.log('[TTS] speakTextGCP 呼び出し:', text.substring(0, 50) + '...', '文字数:', text.length);

      if (!isTTSEnabled || !text) return;

      if (stopPrevious) {
        stopCurrentAudio();
      }

      // マークダウン記号を除去
      const cleanText = stripMarkdown(text);
      
      console.log('[TTS] cleanText:', cleanText.substring(0, 50) + '...', '文字数:', cleanText.length);

      try {
        voiceStatus.innerHTML = '&#128266; 音声合成中...';
        voiceStatus.className = 'voice-status speaking';

        const response = await fetch(`${apiBase}/api/tts/synthesize`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            text: cleanText,
            language_code: 'ja-JP',
            voice_name: 'ja-JP-Chirp3-HD-Leda'
          })
        });

        const data = await response.json();
        
        console.log('[TTS] API レスポンス受信:', data.success);

        if (data.success && data.audio) {
          const audio = new Audio(`data:audio/mp3;base64,${data.audio}`);
          currentAudio = audio;
          
          console.log('[TTS] 音声再生開始');

          // 音声再生完了を待つPromiseを作成
          const playPromise = new Promise<void>((resolve) => {
            audio.onended = () => {
              console.log('[TTS] 音声再生完了');
              voiceStatus.innerHTML = '&#127908; 音声認識: 停止中';
              voiceStatus.className = 'voice-status stopped';
              currentAudio = null;
              resolve();
            };

            audio.onerror = () => {
              console.log('[TTS] 音声再生エラー');
              voiceStatus.innerHTML = '&#127908; 音声認識: 停止中';
              voiceStatus.className = 'voice-status stopped';
              resolve();
            };
          });

          if (isUserInteracted) {
            await audio.play();
            await playPromise; // 再生完了を待つ
          } else {
            showClickPrompt();
            voiceStatus.innerHTML = '&#127908; 音声認識: 停止中';
            voiceStatus.className = 'voice-status stopped';
          }
        }
      } catch (error) {
        console.error('[TTS] Error:', error);
        voiceStatus.innerHTML = '&#127908; 音声認識: 停止中';
        voiceStatus.className = 'voice-status stopped';
      }
    }

    // 無音検出の停止
    function stopVAD() {
      if (vadCheckInterval) {
        clearInterval(vadCheckInterval);
        vadCheckInterval = null;
      }
      if (silenceTimer) {
        clearTimeout(silenceTimer);
        silenceTimer = null;
      }
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }
      analyser = null;
      hasSpoken = false;
    }

    // 無音検出によるオートストップ
    function autoStopRecording() {
      console.log('[VAD] 無音検出 - 自動停止');
      stopVAD();
      if (mediaRecorder && mediaRecorder.state === 'recording') {
        mediaRecorder.stop();
      }
      if (recordingTimer) {
        clearTimeout(recordingTimer);
        recordingTimer = null;
      }
      isRecording = false;
      micBtn.classList.remove('recording');
      micBtn.innerHTML = '&#127908;';
    }

    // 録音切り替え（案2: WebSocket Streaming STT対応）
    async function toggleRecording() {
      enableAudioPlayback();
      stopCurrentAudio();

      if (isRecording) {
        // 録音停止
        stopStreamingSTT();
        return;
      }

      // WebSocket Streaming STT が有効な場合は使用、無効な場合はフォールバック
      if (isStreamingSTT) {
        await startStreamingSTT();
      } else {
        // フォールバック: 従来のMediaRecorder方式
        console.warn('[Recording] WebSocket未接続、フォールバック使用');
        await startLegacyRecording();
      }
    }

    // WebSocket Streaming STT開始
    async function startStreamingSTT() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            channelCount: 1,
            sampleRate: 16000,  // 16kHz (Google Cloud STT推奨)
            echoCancellation: true,
            noiseSuppression: true
          }
        });

        // AudioContext作成（16kHz）
        audioContext = new AudioContext({ sampleRate: 16000 });
        const source = audioContext.createMediaStreamSource(stream);

        // AudioWorklet読み込み
        await audioContext.audioWorklet.addModule('/audio-processor.js');

        // AudioWorkletNode作成
        audioWorkletNode = new AudioWorkletNode(audioContext, 'audio-processor');

        // PCMチャンクをWebSocket経由で送信
        let chunkCount = 0;
        audioWorkletNode.port.onmessage = (event) => {
          const { audioChunk } = event.data;

          // Int16Array をbase64エンコード
          const uint8Array = new Uint8Array(audioChunk.buffer);
          const base64 = btoa(String.fromCharCode.apply(null, Array.from(uint8Array)));

          // WebSocket送信
          if (socket && socket.connected) {
            socket.emit('audio_chunk', { chunk: base64 });
            chunkCount++;
            if (chunkCount % 10 === 0) {
              console.log(`[Audio] チャンク送信: ${chunkCount}個 (${audioChunk.length} samples)`);
            }
          } else {
            console.error('[Audio] WebSocket未接続 - チャンク送信失敗');
          }
        };

        // 音声フロー接続
        source.connect(audioWorkletNode);
        audioWorkletNode.connect(audioContext.destination);

        // VADセットアップ（無音検出）
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 512;
        source.connect(analyser);

        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        hasSpoken = false;
        recordingStartTime = Date.now(); // 録音開始時刻を記録

        vadCheckInterval = window.setInterval(() => {
          if (!analyser || !isRecording) return;

          // 最小録音時間に達していない場合はスキップ
          const elapsed = Date.now() - recordingStartTime;
          if (elapsed < MIN_RECORDING_TIME) {
            return;
          }

          analyser.getByteFrequencyData(dataArray);
          const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;

          if (average > SILENCE_THRESHOLD) {
            hasSpoken = true;
            if (silenceTimer) {
              clearTimeout(silenceTimer);
              silenceTimer = null;
            }
            voiceStatus.innerHTML = '&#127908; 録音中...';
          } else if (hasSpoken && !silenceTimer) {
            voiceStatus.innerHTML = '&#127908; 認識待機中...';
            silenceTimer = window.setTimeout(() => {
              console.log('[VAD] 無音検出 - 自動停止');
              stopStreamingSTT();
            }, SILENCE_DURATION);
          }
        }, 100);

        // WebSocketストリーム開始通知
        socket.emit('start_stream', { language_code: 'ja-JP' });

        isRecording = true;
        micBtn.classList.add('recording');
        micBtn.innerHTML = '&#9209;';
        voiceStatus.innerHTML = '&#127908; 話してください...';
        voiceStatus.className = 'voice-status listening';

        console.log('[WebSocket STT] ストリーミング録音開始');

        // 55秒後に自動停止
        recordingTimer = window.setTimeout(() => {
          if (isRecording) {
            console.log('[Recording] 最大録音時間に達したため自動停止');
            stopStreamingSTT();
            addMessage('system', '録音時間が上限（55秒）に達したため自動停止しました');
          }
        }, MAX_RECORDING_TIME);

      } catch (error) {
        console.error('[WebSocket STT] 録音開始エラー:', error);
        addMessage('system', `マイクアクセスエラー: ${(error as Error).message}`);
      }
    }

    // WebSocket Streaming STT停止
    function stopStreamingSTT() {
      // VAD停止
      stopVAD();

      // AudioWorklet停止
      if (audioWorkletNode) {
        audioWorkletNode.disconnect();
        audioWorkletNode = null;
      }

      // AudioContext停止
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }

      // WebSocketストリーム停止通知
      if (socket && socket.connected) {
        socket.emit('stop_stream');
      }

      // タイマー停止
      if (recordingTimer) {
        clearTimeout(recordingTimer);
        recordingTimer = null;
      }

      isRecording = false;
      micBtn.classList.remove('recording');
      micBtn.innerHTML = '&#127908;';

      console.log('[WebSocket STT] ストリーミング録音停止');
    }

    // フォールバック: 従来のMediaRecorder方式
    async function startLegacyRecording() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' });

        audioChunks = [];
        hasSpoken = false;
        recordingStartTime = Date.now(); // 録音開始時刻を記録

        // 無音検出のセットアップ
        audioContext = new AudioContext();
        const source = audioContext.createMediaStreamSource(stream);
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 512;
        source.connect(analyser);

        const dataArray = new Uint8Array(analyser.frequencyBinCount);

        vadCheckInterval = window.setInterval(() => {
          if (!analyser || !isRecording) return;

          // 最小録音時間に達していない場合はスキップ
          const elapsed = Date.now() - recordingStartTime;
          if (elapsed < MIN_RECORDING_TIME) {
            return;
          }

          analyser.getByteFrequencyData(dataArray);
          const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;

          if (average > SILENCE_THRESHOLD) {
            hasSpoken = true;
            if (silenceTimer) {
              clearTimeout(silenceTimer);
              silenceTimer = null;
            }
            voiceStatus.innerHTML = '&#127908; 録音中...';
          } else if (hasSpoken && !silenceTimer) {
            voiceStatus.innerHTML = '&#127908; 認識待機中...';
            silenceTimer = window.setTimeout(() => {
              autoStopRecording();
            }, SILENCE_DURATION);
          }
        }, 100);

        mediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            audioChunks.push(event.data);
          }
        };

        mediaRecorder.onstop = async () => {
          stopVAD();
          stream.getTracks().forEach(track => track.stop());

          if (recordingTimer) {
            clearTimeout(recordingTimer);
            recordingTimer = null;
          }

          if (audioChunks.length > 0) {
            const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
            await transcribeAudio(audioBlob);
          }
        };

        mediaRecorder.start();
        isRecording = true;
        micBtn.classList.add('recording');
        micBtn.innerHTML = '&#9209;';
        voiceStatus.innerHTML = '&#127908; 話してください...';
        voiceStatus.className = 'voice-status listening';

        recordingTimer = window.setTimeout(() => {
          if (isRecording && mediaRecorder && mediaRecorder.state === 'recording') {
            console.log('[Recording] 最大録音時間に達したため自動停止');
            stopVAD();
            mediaRecorder.stop();
            isRecording = false;
            micBtn.classList.remove('recording');
            micBtn.innerHTML = '&#127908;';
            addMessage('system', '録音時間が上限（55秒）に達したため自動停止しました');
          }
        }, MAX_RECORDING_TIME);

      } catch (error) {
        console.error('[Recording] Error:', error);
        addMessage('system', `マイクアクセスエラー: ${(error as Error).message}`);
      }
    }

    // 音声認識
    async function transcribeAudio(audioBlob: Blob) {
      try {
        voiceStatus.innerHTML = '&#128260; 音声認識中...';
        voiceStatus.className = 'voice-status';

        const reader = new FileReader();
        reader.readAsDataURL(audioBlob);

        reader.onloadend = async () => {
          const base64Audio = (reader.result as string).split(',')[1];

          // 一時的にバッチSTTに戻す（動作確認用）
          const response = await fetch(`${apiBase}/api/stt/transcribe`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              audio: base64Audio,
              language_code: 'ja-JP'
            })
          });

          const data = await response.json();

          if (data.success && data.transcript) {
            userInput.value = data.transcript;
            voiceStatus.innerHTML = '&#9989; 認識完了';
            voiceStatus.className = 'voice-status';

            // ユーザーメッセージを追加
            addMessage('user', data.transcript);

            // 第1段階：即答
            const ack = selectSmartAcknowledgment(data.transcript);
            console.log(`[即答] ${ack.logText}`);

            const preGeneratedAudio = preGeneratedAcks.get(ack.text);
            if (preGeneratedAudio && isTTSEnabled && isUserInteracted) {
              // 事前生成音声を即座に再生（TTSリクエスト不要！）
              const audio = new Audio(`data:audio/mp3;base64,${preGeneratedAudio}`);
              currentAudio = audio;

              audio.onended = () => {
                currentAudio = null;
              };

              audio.play().catch(err => {
                console.error('[即答] 音声再生エラー:', err);
              });

              console.log(`[即答] 事前生成音声を再生: "${ack.text}"`);
            } else if (isTTSEnabled) {
              // フォールバック: 事前生成がない場合はその場で生成
              console.log(`[即答] フォールバック: その場で生成 "${ack.text}"`);
              speakTextGCP(ack.text, false);
            }

            // チャットに即答メッセージを追加
            addMessage('assistant', ack.text);

            // 第2段階：シンプルなオウム返し応答
            const fallbackResponse = generateFallbackResponse(data.transcript);
            if (isTTSEnabled && isUserInteracted) {
              await speakTextGCP(fallbackResponse, false);
            }
            addMessage('assistant', fallbackResponse);

            // 3秒後に追加の応答を再生
            setTimeout(async () => {
              const additionalResponse = "只今、お店の情報を確認中です。もう少々お待ちください。";
              if (isTTSEnabled && isUserInteracted) {
                await speakTextGCP(additionalResponse, false);
              }
              addMessage('assistant', additionalResponse);
            }, 3000);

            // 並列でメッセージ送信（待たない）
            if (!isRecording && userInput.value.trim()) {
              isFromVoiceInput = true; // フラグをセット
              sendMessage(); // awaitしない = 並列実行
            }

            voiceStatus.innerHTML = '&#127908; 音声認識: 停止中';
            voiceStatus.className = 'voice-status stopped';
          } else {
            voiceStatus.innerHTML = '音声が認識されませんでした';
          }
        };

      } catch (error) {
        console.error('[STT] Error:', error);
        addMessage('system', `音声認識エラー: ${(error as Error).message}`);
        voiceStatus.innerHTML = '&#127908; 音声認識: 停止中';
        voiceStatus.className = 'voice-status stopped';
      }
    }

    // スマート即答選択（案3: test_voice_conversation.py準拠）
    function selectSmartAcknowledgment(userMessage: string): { text: string, logText: string } {
      const messageLower = userMessage.trim();

      // 質問形式（明確な疑問文）
      if (/ございますか|でしょうか|いかがですか|ありますか/.test(messageLower)) {
        return {
          text: "確認しますので、少々お待ちください。",
          logText: "質問形式 → 「確認しますので、少々お待ちください。」"
        };
      }

      // 場所・エリアの質問
      if (/どこ|場所|エリア|地域|駅/.test(messageLower)) {
        return {
          text: "お調べします。",
          logText: "場所の質問 → 「お調べします。」"
        };
      }

      // 検索・探す
      if (/探して|探し|教えて|おすすめ|紹介/.test(messageLower)) {
        return {
          text: "かしこまりました。",
          logText: "検索依頼 → 「かしこまりました。」"
        };
      }

      // デフォルト: シンプルな確認
      return {
        text: "はい、承知しました。",
        logText: "デフォルト → 「はい、承知しました。」"
      };
    }

    // TTS切り替え
    function toggleTTS() {
      if (!isUserInteracted) {
        enableAudioPlayback();
        return;
      }

      enableAudioPlayback();
      isTTSEnabled = !isTTSEnabled;
      speakerBtn.innerHTML = isTTSEnabled ? '&#128266;' : '&#128263;';
      speakerBtn.title = isTTSEnabled ? '音声読み上げON' : '音声読み上げOFF';
      speakerBtn.className = isTTSEnabled ? 'btn btn-icon btn-speaker' : 'btn btn-icon btn-speaker disabled';

      if (!isTTSEnabled) {
        stopCurrentAudio();
      }
    }

    // シンプルなオウム返し応答
    function generateFallbackResponse(text: string): string {
      return `"${text}"とのこと。お調べしますので、少々お待ちください。`;
    }

    // 初期化
    async function initialize() {
      try {
        const response = await fetch(`${apiBase}/api/session/start`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ user_info: {} })
        });

        const data = await response.json();
        sessionId = data.session_id;

        addMessage('assistant', data.initial_message);

        // 案1: 即答用音声を事前生成（並列実行）
        console.log('[初期化] 即答用音声を事前生成中...');
        const ackTexts = [
          "確認しますので、少々お待ちください。",
          "お調べします。",
          "かしこまりました。",
          "はい、承知しました。"
        ];

        const ackPromises = ackTexts.map(async (text) => {
          try {
            const ackResponse = await fetch(`${apiBase}/api/tts/synthesize`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                text: text,
                language_code: 'ja-JP',
                voice_name: 'ja-JP-Chirp3-HD-Leda'
              })
            });
            const ackData = await ackResponse.json();
            if (ackData.success && ackData.audio) {
              preGeneratedAcks.set(text, ackData.audio);
              console.log(`[初期化] ? 事前生成完了: "${text}"`);
            }
          } catch (error) {
            console.error(`[初期化] 事前生成失敗: "${text}"`, error);
          }
        });

        // 初期メッセージのTTSと並列実行
        await Promise.all([
          speakTextGCP(data.initial_message),
          ...ackPromises
        ]);

        userInput.disabled = false;
        sendBtn.disabled = false;
        micBtn.disabled = false;
        speakerBtn.disabled = false;
        userInput.focus();

        console.log(`[初期化] 完了 (事前生成音声: ${preGeneratedAcks.size}件)`);

        // 案2: WebSocket Streaming STT接続を初期化
        initializeWebSocketSTT();

      } catch (error) {
        console.error('初期化エラー:', error);
        showError('初期化に失敗しました。ページを再読み込みしてください。');
      }
    }

    // WebSocket Streaming STT接続初期化（案2: 完全実装）
    function initializeWebSocketSTT() {
      try {
        // Socket.IO接続（apiBaseからWebSocketサーバーURLを生成）
        const wsUrl = apiBase || window.location.origin;
        socket = io(wsUrl);

        socket.on('connect', () => {
          console.log('[WebSocket STT] 接続成功');
          isStreamingSTT = true;
        });

        socket.on('disconnect', () => {
          console.log('[WebSocket STT] 切断');
          isStreamingSTT = false;
        });

        socket.on('connected', (data: any) => {
          console.log('[WebSocket STT] サーバー準備完了:', data);
        });

        socket.on('stream_started', (data: any) => {
          console.log('[WebSocket STT] ストリーム開始:', data);
        });

        socket.on('transcript', (data: any) => {
          // リアルタイム認識結果
          const { text, is_final, confidence } = data;

          if (is_final) {
            // 最終結果: ユーザー入力として確定
            console.log(`[WebSocket STT] 最終認識: "${text}" (信頼度: ${confidence.toFixed(2)})`);
            streamingTranscript = text;

            // 音声認識完了処理
            handleStreamingSTTComplete(text);
          } else {
            // 途中結果: リアルタイム表示
            console.log(`[WebSocket STT] 途中認識: "${text}"`);
            userInput.value = text; // リアルタイムで入力欄に表示
          }
        });

        socket.on('stream_stopped', (data: any) => {
          console.log('[WebSocket STT] ストリーム停止:', data);
        });

        socket.on('error', (data: any) => {
          console.error('[WebSocket STT] エラー:', data.message);
          addMessage('system', `音声認識エラー: ${data.message}`);
        });

      } catch (error) {
        console.error('[WebSocket STT] 初期化エラー:', error);
        isStreamingSTT = false;
      }
    }

    // WebSocket Streaming STT完了処理
    function handleStreamingSTTComplete(transcript: string) {
      // まずストリーミングを完全停止
      stopStreamingSTT();
      
      voiceStatus.innerHTML = '? 認識完了';
      voiceStatus.className = 'voice-status';

      // 入力欄に認識結果を設定
      userInput.value = transcript;

      // ユーザーメッセージを追加
      addMessage('user', transcript);

      // 第1段階：即答
      const ack = selectSmartAcknowledgment(transcript);
      console.log(`[即答] ${ack.logText}`);

      const preGeneratedAudio = preGeneratedAcks.get(ack.text);
      let firstAckPromise: Promise<void> | null = null;

      if (preGeneratedAudio && isTTSEnabled && isUserInteracted) {
        // 事前生成音声を即座に再生(TTSリクエスト不要!)
        firstAckPromise = new Promise<void>((resolve) => {
          const audio = new Audio(`data:audio/mp3;base64,${preGeneratedAudio}`);
          currentAudio = audio;

          audio.onended = () => {
            currentAudio = null;
            resolve();
          };

          audio.play().catch(err => {
            console.error('[即答] 音声再生エラー:', err);
            resolve();
          });
        });
        console.log(`[即答] 事前生成音声を再生: "${ack.text}"`);
      } else if (isTTSEnabled) {
        // フォールバック: 事前生成がない場合はその場で生成
        console.log(`[即答] フォールバック: その場で生成 "${ack.text}"`);
        firstAckPromise = speakTextGCP(ack.text, false);
      }

      // チャットに即答メッセージを追加
      addMessage('assistant', ack.text);

      // 録音を停止
      stopStreamingSTT();

      // 第2段階：シンプルなオウム返し応答
      (async () => {
        try {
          // 第1段階の即答が終わるのを待つ
          if (firstAckPromise) {
            await firstAckPromise;
          }

          // シンプルなオウム返し応答
          const fallbackResponse = generateFallbackResponse(transcript);
          console.log(`[フォールバック応答] "${transcript}" → "${fallbackResponse}"`);

          if (isTTSEnabled && isUserInteracted) {
            await speakTextGCP(fallbackResponse, false);
          }
          addMessage('assistant', fallbackResponse);

          // 5秒後に追加の応答を再生
          setTimeout(async () => {
            const additionalResponse = "只今、お店の情報を確認中です。もう少々お待ちください。";
            if (isTTSEnabled && isUserInteracted) {
              await speakTextGCP(additionalResponse, false);
            }
            addMessage('assistant', additionalResponse);
          }, 5000);

          // 並列でメッセージ送信
          if (userInput.value.trim()) {
            isFromVoiceInput = true;
            sendMessage();
          }

        } catch (error) {
          console.error('第2段階応答生成エラー:', error);
          // エラー時はシンプルな応答でフォールバック
          const fallbackResponse = generateFallbackResponse(transcript);
          if (isTTSEnabled && isUserInteracted) {
            await speakTextGCP(fallbackResponse, false);
          }
          addMessage('assistant', fallbackResponse);

          if (userInput.value.trim()) {
            isFromVoiceInput = true;
            sendMessage();
          }
        }
      })();

      voiceStatus.innerHTML = '?? 音声認識: 停止中';
      voiceStatus.className = 'voice-status stopped';
    }

    // レスポンステキストからショップ情報を抽出
    function extractShopsFromResponse(text: string): any[] {
      const shops: any[] = [];
      
      // パターン: 番号. **店名（読み）**: 説明
      // または: 番号. **店名**: 説明
      const pattern = /(\d+)\.\s*\*\*([^*]+)\*\*[：:]\s*([^\n]+)/g;
      let match;

      console.log('[ShopExtract] テキストからショップを抽出中...');

      while ((match = pattern.exec(text)) !== null) {
        const fullName = match[2].trim();
        const description = match[3].trim();
        
        // 店名から読み仮名を分離
        let name = fullName;
        let reading = '';
        
        const nameMatch = fullName.match(/^([^（(]+)[（(]([^）)]+)[）)]/);
        if (nameMatch) {
          name = nameMatch[1].trim();
          reading = nameMatch[2].trim();
        }

        // 店名をエンコードしてURLを生成
        const encodedName = encodeURIComponent(name);

        shops.push({
          name: name,
          description: description,
          category: 'イタリアン',
          // 検索用URLを生成
          hotpepper_url: `https://www.hotpepper.jp/SA11/srchRS/?keyword=${encodedName}`,
          maps_url: `https://www.google.com/maps/search/${encodedName}`,
          tabelog_url: `https://tabelog.com/rstLst/?vs=1&sa=&sk=${encodedName}`
        });
      }

      console.log(`[ShopExtract] ${shops.length}件のショップを抽出:`, shops);

      return shops;
    }

    // メッセージ送信
    async function sendMessage() {
      enableAudioPlayback();

      const message = userInput.value.trim();
      if (!message || isProcessing) return;

      isProcessing = true;
      sendBtn.disabled = true;
      micBtn.disabled = true;
      userInput.disabled = true;

      // テキスト送信の場合のみユーザーメッセージを追加
      // （音声の場合はtranscribeAudio()で既にユーザーメッセージ追加済み）
      if (!isFromVoiceInput) {
        addMessage('user', message);
      }
      userInput.value = '';

      // 案1: 即答は transcribeAudio() で既に再生済み（音声入力の場合）
      // テキスト入力の場合のみここで即答を生成
      if (!isFromVoiceInput) {
        const ack = selectSmartAcknowledgment(message);
        console.log(`[即答/テキスト] ${ack.logText}`);

        const preGeneratedAudio = preGeneratedAcks.get(ack.text);
        if (preGeneratedAudio && isTTSEnabled && isUserInteracted) {
          const audio = new Audio(`data:audio/mp3;base64,${preGeneratedAudio}`);
          currentAudio = audio;
          audio.onended = () => { currentAudio = null; };
          audio.play().catch(err => console.error('[即答] 音声再生エラー:', err));
        }
        addMessage('assistant', ack.text);
      }

      // フラグをリセット
      const wasVoiceInput = isFromVoiceInput;
      isFromVoiceInput = false;

      try {
        const loadingId = addLoadingMessage();

        const response = await fetch(`${apiBase}/api/chat`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            session_id: sessionId,
            message: message,
            stage: currentStage
          })
        });

        const data = await response.json();

        removeLoadingMessage(loadingId);

        // デバッグ: レスポンス内容を確認
        console.log('[DEBUG] data.response 文字数:', data.response.length);
        console.log('[DEBUG] data.response 内容:', data.response);
        console.log('[DEBUG] data.summary:', data.summary);

        // 本文を表示（summaryも保存するが読み上げはresponseのみ）
        addMessage('assistant', data.response, data.summary);

        // 先行TTSを停止
        stopCurrentAudio();

        // ショップデータの有無で読み上げ内容を変える
        if (data.shops && data.shops.length > 0) {
          // ショップデータを保存し、予約依頼ボタンを有効化
          currentShops = data.shops;
          reservationBtn.disabled = false;

          // 詳細情報の音声合成をバックグラウンドで開始（並列実行）
          const synthesisPromise = (async () => {
            try {
              const cleanText = stripMarkdown(data.response);
              
              const response = await fetch(`${apiBase}/api/tts/synthesize`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                  text: cleanText,
                  language_code: 'ja-JP',
                  voice_name: 'ja-JP-Chirp3-HD-Leda'
                })
              });

              const synthesisResult = await response.json();
              
              if (synthesisResult.success && synthesisResult.audio) {
                return `data:audio/mp3;base64,${synthesisResult.audio}`;
              }
            } catch (error) {
              console.error('[並列音声合成] エラー:', error);
            }
            return null;
          })();

          // 短い定型文を即座に再生（完了まで待つ）
          await speakTextGCP(`お待たせしました。${data.shops.length}件のお店をご案内します。`);

          // 合成済みの音声を再生
          const audioData = await synthesisPromise;
          
          if (audioData && isTTSEnabled && isUserInteracted) {
            try {
              stopCurrentAudio();
              const audio = new Audio(audioData);
              currentAudio = audio;
              
              audio.onended = () => {
                currentAudio = null;
                voiceStatus.innerHTML = '&#127908; 音声認識: 停止中';
                voiceStatus.className = 'voice-status stopped';
              };

              voiceStatus.innerHTML = '&#128266; 音声再生中...';
              voiceStatus.className = 'voice-status speaking';

              await audio.play();
            } catch (error) {
              console.error('[並列音声再生] エラー:', error);
              voiceStatus.innerHTML = '&#127908; 音声認識: 停止中';
              voiceStatus.className = 'voice-status stopped';
            }
          }

          // ショップカード表示
          const event = new CustomEvent('displayShops', {
            detail: { shops: data.shops }
          });
          document.dispatchEvent(event);

          // セクションにクラスを追加
          const section = document.getElementById('shopListSection');
          if (section) {
            section.classList.add('has-shops');
          }
        } else {
          // ショップデータがない場合の処理
          if (data.response) {
            // レスポンスからショップ情報をパースして抽出を試みる
            const extractedShops = extractShopsFromResponse(data.response);
            if (extractedShops.length > 0) {
              // ショップデータを保存し、予約依頼ボタンを有効化
              currentShops = extractedShops;
              reservationBtn.disabled = false;

              // 短い定型文を即座に再生（完了まで待つ）
              await speakTextGCP(`お待たせしました。${extractedShops.length}件のお店をご案内します。`);

              // 詳細情報の音声合成をバックグラウンドで開始（並列実行）
              const synthesisPromise = (async () => {
                try {
                  const cleanText = stripMarkdown(data.response);
                  
                  const response = await fetch(`${apiBase}/api/tts/synthesize`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                      text: cleanText,
                      language_code: 'ja-JP',
                      voice_name: 'ja-JP-Chirp3-HD-Leda'
                    })
                  });

                  const synthesisResult = await response.json();
                  
                  if (synthesisResult.success && synthesisResult.audio) {
                    return `data:audio/mp3;base64,${synthesisResult.audio}`;
                  }
                } catch (error) {
                  console.error('[並列音声合成] エラー:', error);
                }
                return null;
              })();

              // 合成済みの音声を再生
              const audioData = await synthesisPromise;
              
              if (audioData && isTTSEnabled && isUserInteracted) {
                try {
                  stopCurrentAudio();
                  const audio = new Audio(audioData);
                  currentAudio = audio;
                  
                  audio.onended = () => {
                    currentAudio = null;
                    voiceStatus.innerHTML = '&#127908; 音声認識: 停止中';
                    voiceStatus.className = 'voice-status stopped';
                  };

                  voiceStatus.innerHTML = '&#128266; 音声再生中...';
                  voiceStatus.className = 'voice-status speaking';

                  await audio.play();
                } catch (error) {
                  console.error('[並列音声再生] エラー:', error);
                  voiceStatus.innerHTML = '&#127908; 音声認識: 停止中';
                  voiceStatus.className = 'voice-status stopped';
                }
              }

              const event = new CustomEvent('displayShops', {
                detail: { shops: extractedShops }
              });
              document.dispatchEvent(event);

              const section = document.getElementById('shopListSection');
              if (section) {
                section.classList.add('has-shops');
              }
            } else {
              // ショップなしの通常会話は全文読み上げ
              speakTextGCP(data.response);
            }
          }
        }
      } catch (error) {
        console.error('送信エラー:', error);
        showError('メッセージの送信に失敗しました。');
} finally {
  isProcessing = false;
  sendBtn.disabled = false;
  micBtn.disabled = false;
  userInput.disabled = false;
  
  // ショップカードが表示される場合はフォーカスしない
  // （スマホでキーボードが出るのを防ぐ）
  if (currentShops.length === 0) {
    userInput.focus();
  } else {
    userInput.blur(); // 明示的にフォーカスを外す
  }
}
    }

    // 予約依頼モーダルを開く
    function openReservationModal() {
      if (currentShops.length === 0) {
        showError('お店を検索してからご利用ください。');
        return;
      }

      // カスタムイベントを発火してモーダルを開く
      const event = new CustomEvent('openReservationModal', {
        detail: { shops: currentShops }
      });
      document.dispatchEvent(event);
    }

    // メッセージ追加
    function addMessage(role: string, content: string, summary: string | null = null) {
      const messageDiv = document.createElement('div');
      messageDiv.className = `message ${role}`;

      const avatar = document.createElement('div');
      avatar.className = 'message-avatar';
      avatar.innerHTML = role === 'assistant' ? '&#127869;' : role === 'user' ? '&#128100;' : '&#9888;';

      const contentDiv = document.createElement('div');
      contentDiv.className = 'message-content';
      contentDiv.textContent = content;

      messageDiv.appendChild(avatar);

      const wrapper = document.createElement('div');
      wrapper.appendChild(contentDiv);

      if (summary) {
        const summaryDiv = document.createElement('div');
        summaryDiv.className = 'summary-box';
        summaryDiv.innerHTML = `<strong>&#128221; 内容確認</strong>${summary}`;
        wrapper.appendChild(summaryDiv);
      }

      messageDiv.appendChild(wrapper);
      chatArea.appendChild(messageDiv);
      chatArea.scrollTop = chatArea.scrollHeight;
    }

    // ローディング表示
    function addLoadingMessage(): string {
      const id = 'loading-' + Date.now();
      const messageDiv = document.createElement('div');
      messageDiv.id = id;
      messageDiv.className = 'message assistant';

      const avatar = document.createElement('div');
      avatar.className = 'message-avatar';
      avatar.innerHTML = '&#127869;';

      const contentDiv = document.createElement('div');
      contentDiv.className = 'message-content';
      contentDiv.innerHTML = `
        <div class="loading"></div>
        <p style="margin-top: 8px; font-size: 13px; color: #6b7280;">
          提案するお店の情報を探しています。少々お待ちください...
        </p>
      `;

      messageDiv.appendChild(avatar);
      messageDiv.appendChild(contentDiv);
      chatArea.appendChild(messageDiv);
      chatArea.scrollTop = chatArea.scrollHeight;

      return id;
    }

    function removeLoadingMessage(id: string) {
      const element = document.getElementById(id);
      if (element) element.remove();
    }

    // 最終要約表示
    function showFinalSummary(summary: string) {
      const summaryDiv = document.createElement('div');
      summaryDiv.className = 'final-summary';
      summaryDiv.innerHTML = `
        <h3>&#128203; 質問要約書</h3>
        <div class="final-summary-content">${summary}</div>
        <p style="margin-top: 12px; font-size: 12px; color: #6b7280;">
          担当スタッフが内容を確認し、追ってご連絡いたします。
        </p>
      `;
      chatArea.appendChild(summaryDiv);
      chatArea.scrollTop = chatArea.scrollHeight;
    }

    // エラー表示
    function showError(message: string) {
      const errorDiv = document.createElement('div');
      errorDiv.className = 'error-message';
      errorDiv.textContent = message;
      chatArea.appendChild(errorDiv);
      chatArea.scrollTop = chatArea.scrollHeight;
    }

    // イベントリスナー
    sendBtn.addEventListener('click', sendMessage);
    micBtn.addEventListener('click', toggleRecording);
    speakerBtn.addEventListener('click', toggleTTS);
    userInput.addEventListener('keypress', (e) => {
      if (e.key === 'Enter') sendMessage();
    });
    reservationBtn.addEventListener('click', openReservationModal);

    // 初期化実行
    initialize();
  });
</script>